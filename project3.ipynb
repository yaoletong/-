{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee40d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text8 dataset already downloaded.\n",
      "Text8 dataset extracted.\n",
      "Vocabulary size: 10000\n",
      "Number of training pairs: 39994\n",
      "cbow Epoch 1, Loss: 99.85436055895724\n",
      "cbow Epoch 2, Loss: 70.25942900866258\n",
      "cbow Epoch 3, Loss: 57.58860725396015\n",
      "cbow Epoch 4, Loss: 49.53454342138034\n",
      "cbow Epoch 5, Loss: 43.82359591462081\n",
      "cbow Epoch 6, Loss: 39.5166662275801\n",
      "cbow Epoch 7, Loss: 36.137674544674006\n",
      "cbow Epoch 8, Loss: 33.40568355017717\n",
      "cbow Epoch 9, Loss: 31.146844948857957\n",
      "cbow Epoch 10, Loss: 29.253593067845692\n",
      "SkipGram Batch 1, Loss: 36.43420736401264\n",
      "SkipGram Batch 11, Loss: 26.990795532322693\n",
      "SkipGram Batch 21, Loss: 23.504581049992495\n",
      "SkipGram Batch 31, Loss: 21.040331318632532\n",
      "SkipGram Epoch: 0, Loss: 100.04570930013863\n",
      "SkipGram Batch 1, Loss: 19.04839011856509\n",
      "SkipGram Batch 11, Loss: 17.941533480461324\n",
      "SkipGram Batch 21, Loss: 17.97567983730538\n",
      "SkipGram Batch 31, Loss: 17.4098148953555\n",
      "SkipGram Epoch: 1, Loss: 70.29445440020517\n",
      "SkipGram Batch 1, Loss: 14.844484805978466\n",
      "SkipGram Batch 11, Loss: 15.105514396748749\n",
      "SkipGram Batch 21, Loss: 14.322434065264149\n",
      "SkipGram Batch 31, Loss: 13.84484920338935\n",
      "SkipGram Epoch: 2, Loss: 57.76801935522428\n",
      "SkipGram Batch 1, Loss: 12.797223952379905\n",
      "SkipGram Batch 11, Loss: 13.193511052288807\n",
      "SkipGram Batch 21, Loss: 12.118234300912603\n",
      "SkipGram Batch 31, Loss: 12.145448107705636\n",
      "SkipGram Epoch: 3, Loss: 49.731008694603936\n",
      "SkipGram Batch 1, Loss: 11.328821114415208\n",
      "SkipGram Batch 11, Loss: 11.11831294892354\n",
      "SkipGram Batch 21, Loss: 11.263153487137592\n",
      "SkipGram Batch 31, Loss: 10.780156166094292\n",
      "SkipGram Epoch: 4, Loss: 43.96893220674525\n",
      "SkipGram Batch 1, Loss: 10.27142694033945\n",
      "SkipGram Batch 11, Loss: 9.690883124161234\n",
      "SkipGram Batch 21, Loss: 9.987412237885273\n",
      "SkipGram Batch 31, Loss: 10.276093625194198\n",
      "SkipGram Epoch: 5, Loss: 39.60287776454066\n",
      "SkipGram Batch 1, Loss: 9.206718733388232\n",
      "SkipGram Batch 11, Loss: 9.13106654717461\n",
      "SkipGram Batch 21, Loss: 8.955303430225753\n",
      "SkipGram Batch 31, Loss: 9.03140872675619\n",
      "SkipGram Epoch: 6, Loss: 36.18947678362879\n",
      "SkipGram Batch 1, Loss: 8.31086383694404\n",
      "SkipGram Batch 11, Loss: 8.648163254177831\n",
      "SkipGram Batch 21, Loss: 8.448949890904547\n",
      "SkipGram Batch 31, Loss: 8.056887052661882\n",
      "SkipGram Epoch: 7, Loss: 33.45597429345534\n",
      "SkipGram Batch 1, Loss: 7.390877785177445\n",
      "SkipGram Batch 11, Loss: 7.713042237851478\n",
      "SkipGram Batch 21, Loss: 7.933043996744646\n",
      "SkipGram Batch 31, Loss: 7.833002686110641\n",
      "SkipGram Epoch: 8, Loss: 31.20617734440604\n",
      "SkipGram Batch 1, Loss: 7.267488832948817\n",
      "SkipGram Batch 11, Loss: 7.4190330521102466\n",
      "SkipGram Batch 21, Loss: 7.214632420060293\n",
      "SkipGram Batch 31, Loss: 7.349526225525821\n",
      "SkipGram Epoch: 9, Loss: 29.317087065574654\n",
      "is的词向量: [ 0.26341677  0.05926219  0.12354635  0.13140331 -0.01257439 -0.10382722\n",
      "  0.00573798  0.25031215 -0.00870509 -0.02238409  0.09957359  0.29691213\n",
      "  0.08917145  0.04381887 -0.00904115  0.02682015 -0.35895056  0.00749872\n",
      " -0.03317474 -0.09504174 -0.05300616 -0.15045561 -0.1439825  -0.12503085\n",
      " -0.07456744  0.0229864   0.09710146 -0.06757426  0.09062415  0.2126125\n",
      "  0.16230898 -0.16735627  0.07085954  0.00254115  0.23333019  0.06224839\n",
      " -0.04309728 -0.34130093  0.19463608  0.20293126  0.2549891   0.03315361\n",
      "  0.08364078 -0.11396331 -0.25016564 -0.02500116  0.11757336  0.01409776\n",
      "  0.2588159   0.21796802 -0.15142243  0.0760098   0.05059543 -0.04358535\n",
      " -0.17222989  0.1208345   0.02370299 -0.16482568  0.14261378 -0.23630561\n",
      "  0.05319823 -0.03003116  0.02767307  0.17063391  0.01097281  0.0086149\n",
      " -0.04609331 -0.30106246  0.24940313 -0.00039449 -0.2002164  -0.06792249\n",
      "  0.1748205   0.0398501  -0.09143624  0.16275549  0.14182922 -0.13547221\n",
      "  0.02945201 -0.15621988 -0.01646874  0.02549915  0.03535904  0.21905103\n",
      "  0.07213634  0.04695045  0.19328031  0.05966931 -0.06514132  0.20508385\n",
      " -0.09449369 -0.03414025 -0.14901046  0.148349    0.03011354  0.06114405\n",
      " -0.05590509  0.21097806 -0.35677776 -0.10756089]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from gensim.models import KeyedVectors\n",
    "# 下载 Text8 数据集\n",
    "def download_text8(data_dir='data'):\n",
    "    url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    zip_path = os.path.join(data_dir, 'text8.zip')\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading text8 dataset...\")\n",
    "        response = requests.get(url)\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"Text8 dataset already downloaded.\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Text8 dataset extracted.\")\n",
    "\n",
    "# 保存词嵌入到文件\n",
    "def save_embeddings(word_embeddings, word_to_idx, vocab_size, embed_size, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.writelines(f\"{vocab_size} {embed_size}\\n\")\n",
    "        for word, idx in word_to_idx.items():\n",
    "            embedding = ' '.join(map(str, word_embeddings[idx]))\n",
    "            f.write(f\"{word} {embedding}\\n\")\n",
    "\n",
    "# 构建词汇表和词索引映射\n",
    "def build_vocab(text, vocab_size=10000):\n",
    "    words = text.split()\n",
    "    vocab = [word for word, _ in Counter(words).most_common(vocab_size)]\n",
    "    \n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "# 转换文本到索引\n",
    "def text_to_indices(text, word_to_idx):\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "# 生成 Skip-gram 训练数据\n",
    "def generate_skipgram_data(indices, window_size=2):\n",
    "    pairs = []\n",
    "    for i in range(len(indices)):\n",
    "        target = indices[i]\n",
    "        context_indices = list(range(max(0, i - window_size), i)) + list(range(i + 1, min(len(indices), i + window_size + 1)))\n",
    "        for j in context_indices:\n",
    "            context = indices[j]\n",
    "            pairs.append((target, context))\n",
    "    return pairs\n",
    "\n",
    "def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "class CBOW: #CBOW模型\n",
    "    def __init__(self, vocab_size, embed_size, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W1 = np.random.randn(vocab_size, embed_size)\n",
    "        self.W2 = np.random.randn(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, context):\n",
    "        self.context_one_hot = np.zeros(self.vocab_size)\n",
    "        self.context_one_hot[context] = 1\n",
    "        h = np.dot(self.W1.T, self.context_one_hot)\n",
    "        u = np.dot(self.W2.T, h)\n",
    "        y_pred = softmax(u)\n",
    "        return y_pred, h, u\n",
    "    \n",
    "    def backward(self, context, target, y_pred, h, learning_rate):\n",
    "        dL_du = y_pred\n",
    "        dL_du[target] -= 1\n",
    "        \n",
    "        dL_dW2 = np.outer(h, dL_du)\n",
    "        dW1 = np.outer(self.context_one_hot, np.dot(self.W2, dL_du.T))\n",
    "        \n",
    "        self.W2 -= learning_rate * dL_dW2\n",
    "        \n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "    \n",
    "    def train(self, pairs, epochs=5):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for target, context in pairs:\n",
    "                y_pred, h, u = self.forward(context)\n",
    "                loss = -np.log(y_pred[target])\n",
    "                total_loss += loss\n",
    "                self.backward(context, target, y_pred, h, self.learning_rate)\n",
    "            print(f'cbow Epoch {epoch + 1}, Loss: {total_loss/10000}')\n",
    "\n",
    "    def get_embedding(self, word_idx):\n",
    "        return self.W1[word_idx]\n",
    "    \n",
    "    def save_embeddings(self, file_name):\n",
    "        np.save(file_name, self.W1)\n",
    "    \n",
    "    def load_embeddings(self, file_name):\n",
    "        self.W1 = np.load(file_name)\n",
    "    \n",
    "\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, vocab_size, embed_size, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W1 = np.random.randn(vocab_size, embed_size)\n",
    "        self.W2 = np.random.randn(embed_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    def train(self, pairs, epochs=5, batch_size=1000):\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(pairs)\n",
    "            loss = 0\n",
    "            for i in range(0, len(pairs), batch_size):\n",
    "                \n",
    "                batch_pairs = pairs[i:i + batch_size]\n",
    "                batch_loss = self.train_batch(batch_pairs)\n",
    "                loss += batch_loss\n",
    "                \n",
    "                if (i // batch_size) % 10 == 0:\n",
    "                    print(f'SkipGram Batch {i // batch_size + 1}, Loss: {batch_loss/1000}')\n",
    "            print(f'SkipGram Epoch: {epoch}, Loss: {loss/10000}')\n",
    "    \n",
    "    def train_batch(self, batch_pairs):\n",
    "        batch_loss = 0\n",
    "        for target, context in batch_pairs:\n",
    "            target_one_hot = np.zeros(self.vocab_size)\n",
    "            target_one_hot[target] = 1\n",
    "            \n",
    "            h = np.dot(self.W1.T, target_one_hot)\n",
    "            u = np.dot(self.W2.T, h)\n",
    "            y_hat = softmax(u) #预测值\n",
    "            \n",
    "            e = y_hat.copy()\n",
    "            e[context] -= 1 # 梯度\n",
    "            \n",
    "            dW2 = np.outer(h, e)\n",
    "            dW1 = np.outer(target_one_hot, np.dot(self.W2, e.T))\n",
    "            \n",
    "            self.W1 -= self.learning_rate * dW1\n",
    "            self.W2 -= self.learning_rate * dW2\n",
    "            \n",
    "            batch_loss += -np.log(y_hat[context])\n",
    "        return batch_loss\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return self.W1\n",
    "    \n",
    "    def save_embeddings(self, file_name):\n",
    "        np.save(file_name, self.W1)\n",
    "    \n",
    "    def load_embeddings(self, file_name):\n",
    "        self.W1 = np.load(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 下载并预处理数据\n",
    "download_text8()\n",
    "with open('data/text8', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "word_to_idx, idx_to_word = build_vocab(text, vocab_size) #构建词汇表\n",
    "indices = text_to_indices(text, word_to_idx)  #文本转索引\n",
    "pairs = generate_skipgram_data(indices[:10000])\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Number of training pairs: {len(pairs)}\")\n",
    "\n",
    "\n",
    "# 训练 模型\n",
    "embed_size = 100\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 1000\n",
    "\n",
    "cbow = CBOW(vocab_size, embed_size, learning_rate) #训练cbow模型\n",
    "cbow.train(pairs, epochs=epochs)\n",
    "\n",
    "skip_gram = SkipGram(vocab_size, embed_size, learning_rate) #训练skipGram模型\n",
    "skip_gram.train(pairs, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "word_embeddings = skip_gram.get_embedding() #获取词嵌入w1\n",
    "save_embeddings(word_embeddings, word_to_idx, vocab_size, embed_size,'word_embeddings.txt')  #保存嵌入\n",
    "\n",
    "loaded_embeddings = KeyedVectors.load_word2vec_format('word_embeddings.txt', binary=False) #加载嵌入\n",
    "\n",
    "# 获取某个词的嵌入\n",
    "print(\"is的词向量:\", loaded_embeddings['is'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64227afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
