{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, logits, labels):\n",
    "        # 计算softmax\n",
    "        self.logits = logits\n",
    "        self.labels = labels\n",
    "        self.probs = self.softmax(logits)\n",
    "        self.loss = -np.sum(labels * np.log(self.probs + 1e-9)) / logits.shape[0]\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self):\n",
    "        d_logits = (self.probs - self.labels) / self.logits.shape[0]\n",
    "        return d_logits\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out * (self.inputs > 0)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "class Conv2D():\n",
    "    def __init__(self,  input_channels, num_filters, kernel_size, stride=1, padding=0):\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_channels = input_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.count = 0\n",
    "        # 初始化滤波器和偏置\n",
    "        self.filters = np.random.randn(num_filters, input_channels, kernel_size, kernel_size) / (kernel_size * kernel_size)\n",
    "        self.biases = np.zeros(num_filters)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.input_padded = np.pad(input, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
    "        \n",
    "        self.output_height = (self.input_padded.shape[2] - self.kernel_size) // self.stride + 1\n",
    "        self.output_width = (self.input_padded.shape[3] - self.kernel_size) // self.stride + 1\n",
    "        self.output_shape = (self.num_filters, self.output_height, self.output_width)\n",
    "        self.output = np.zeros((input.shape[0],) + self.output_shape)\n",
    "\n",
    "        for i in range(self.output_height):\n",
    "            for j in range(self.output_width):\n",
    "                for f in range(self.num_filters):\n",
    "                    vertical_start = i * self.stride\n",
    "                    vertical_end = vertical_start + self.kernel_size\n",
    "                    horizontal_start = j * self.stride\n",
    "                    horizontal_end = horizontal_start + self.kernel_size\n",
    "                    region = self.input_padded[:, :, vertical_start:vertical_end, horizontal_start:horizontal_end]\n",
    "                    self.output[:, f, i, j] = np.sum(region * self.filters[f], axis=(1, 2, 3)) + self.biases[f]\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        self.count += 1\n",
    "        d_filters = np.zeros(self.filters.shape)\n",
    "        d_biases = np.zeros(self.biases.shape)\n",
    "        d_input_padded = np.zeros(self.input_padded.shape)\n",
    "\n",
    "        for i in range(self.output_height):\n",
    "            for j in range(self.output_width):\n",
    "                for f in range(self.num_filters):\n",
    "                    vertical_start = i * self.stride\n",
    "                    vertical_end = vertical_start + self.kernel_size\n",
    "                    horizontal_start = j * self.stride\n",
    "                    horizontal_end = horizontal_start + self.kernel_size\n",
    "\n",
    "                    region = self.input_padded[:, :, vertical_start:vertical_end, horizontal_start:horizontal_end]\n",
    "\n",
    "                    for b in range(d_out.shape[0]):\n",
    "                        d_filters[f] += d_out[b, f, i, j] * region[b]\n",
    "                        d_biases[f] += d_out[b, f, i, j]\n",
    "                        d_input_padded[b, :, vertical_start:vertical_end, horizontal_start:horizontal_end] += d_out[b, f, i, j] * self.filters[f]\n",
    "        \n",
    "        d_input = d_input_padded[:, :, self.padding:self.input_padded.shape[2]-self.padding, self.padding:self.input_padded.shape[3]-self.padding]\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.filters -= learning_rate * d_filters\n",
    "        self.biases -= learning_rate * d_biases\n",
    "\n",
    "        return d_input\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, num_features, momentum=0.9, epsilon=1e-5):\n",
    "        self.num_features = num_features\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # 初始化缩放和平移参数\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        \n",
    "        # 运行均值和方差\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.batch_mean = np.mean(X, axis=0) # 计算均值 三维\n",
    "            self.batch_var = np.var(X, axis=0) # 计算方差 三维\n",
    "            \n",
    "            self.X_normalized = (X - self.batch_mean) / np.sqrt(self.batch_var + self.epsilon)\n",
    "            self.out = self.gamma * self.X_normalized + self.beta\n",
    "            \n",
    "            # 更新运行均值和方差\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_var\n",
    "        else:\n",
    "            self.X_normalized = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.out = self.gamma * self.X_normalized + self.beta\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, d_out ,learning_rate = 0.001):\n",
    "        N, D = d_out.shape\n",
    "        \n",
    "        X_mu = self.X_normalized * np.sqrt(self.batch_var + self.epsilon)\n",
    "        \n",
    "        dbeta = np.sum(d_out, axis=0)\n",
    "        dgamma = np.sum(d_out * self.X_normalized, axis=0)\n",
    "        \n",
    "        dX_normalized = d_out * self.gamma\n",
    "        dvar = np.sum(dX_normalized * X_mu * -0.5 * (self.batch_var + self.epsilon) ** -1.5, axis=0)\n",
    "        dmean = np.sum(dX_normalized * -1 / np.sqrt(self.batch_var + self.epsilon), axis=0) + dvar * np.mean(-2 * X_mu, axis=0)\n",
    "        \n",
    "        dX = (dX_normalized / np.sqrt(self.batch_var + self.epsilon)) + (dvar * 2 * X_mu / N) + (dmean / N)\n",
    "        \n",
    "        # 更新参数\n",
    "        self.gamma -= learning_rate * dgamma\n",
    "        self.beta -= learning_rate * dbeta\n",
    "        \n",
    "        return dX\n",
    "\n",
    "class MaxPooling2D:\n",
    "    def __init__(self, pool_size, stride=None):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride if stride is not None else pool_size\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        batch_size, num_channels, input_height, input_width = inputs.shape\n",
    "        self.output_height = (input_height - self.pool_size) // self.stride + 1\n",
    "        self.output_width = (input_width - self.pool_size) // self.stride + 1\n",
    "        self.output = np.zeros((batch_size, num_channels, self.output_height, self.output_width))\n",
    "\n",
    "        for i in range(self.output_height):\n",
    "            for j in range(self.output_width):\n",
    "                input_slice = inputs[:, :, i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size]\n",
    "                self.output[:, :, i, j] = np.max(input_slice, axis=(2, 3))\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_input = np.zeros_like(self.inputs)\n",
    "        for i in range(self.output_height):\n",
    "            for j in range(self.output_width):\n",
    "                input_slice = self.inputs[:, :, i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size]\n",
    "                mask = (input_slice == np.max(input_slice, axis=(2, 3))[:, :, None, None])\n",
    "            d_input[:, :, i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size] += mask * d_out[:, :, i, j][:, :, None, None]\n",
    "\n",
    "        return d_input\n",
    "\n",
    "class Flatten:\n",
    "    def forward(self, inputs):\n",
    "        self.input_shape = inputs.shape\n",
    "        return inputs.reshape(self.input_shape[0], -1)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out.reshape(self.input_shape)\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, input_units, output_units):\n",
    "        self.weights = np.random.randn(input_units, output_units) * np.sqrt(2.0 / input_units)\n",
    "        self.biases = np.zeros((1, output_units))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        self.weight_gradients = np.dot(self.inputs.T, d_out)\n",
    "        self.bias_gradients = np.sum(d_out, axis=0, keepdims=True)\n",
    "        self.weights -= learning_rate * self.weight_gradients\n",
    "        self.biases -= learning_rate * self.bias_gradients\n",
    "        return d_input\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_prob):\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def forward(self, inputs, is_training=True):\n",
    "        if is_training:\n",
    "            self.mask = (np.random.rand(*inputs.shape) < self.dropout_prob) / self.dropout_prob\n",
    "            self.output = inputs * self.mask\n",
    "        else:\n",
    "            self.output = inputs\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask\n",
    "    \n",
    "\n",
    "\n",
    "# Define the CNN model with BatchNorm and Dropout\n",
    "class CNN():\n",
    "    def __init__(self,num_conv_layers, num_dense_layers):\n",
    "\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.sequence_conv_layers = [] #卷积层列表\n",
    "        input_channels = 3\n",
    "\n",
    "        if self.num_conv_layers == -1:#Alexnet模型\n",
    "            \"\"\" self.conv1 = Conv2D(input_channels, 6, kernel_size=11, stride=4, padding=0)\n",
    "            self.relu1 = ReLU()\n",
    "            self.maxpool1 = MaxPooling2D(pool_size=3, stride=2)\n",
    "            self.conv2 = Conv2D(6, 8, kernel_size=5, stride=1, padding=2)\n",
    "            self.relu2 = ReLU()\n",
    "            self.maxpool2 = MaxPooling2D(pool_size=3, stride=2)\n",
    "            self.conv3 = Conv2D(8, 8, kernel_size=3, stride=1, padding=1)\n",
    "            self.relu3 = ReLU()\n",
    "            self.conv4 = Conv2D(8, 8, kernel_size=3, stride=1, padding=1)\n",
    "            self.relu4 = ReLU()\n",
    "            self.conv5 = Conv2D(8, 10, kernel_size=3, stride=1, padding=1)\n",
    "            self.relu5 = ReLU()\n",
    "            self.maxpool3 = MaxPooling2D(pool_size=3, stride=2)\n",
    "            self.flatten = Flatten() \"\"\"\n",
    "            self.sequence_conv_layers.append(Conv2D(input_channels, 96, kernel_size=11, stride=4, padding=0))  #55*55\n",
    "            self.sequence_conv_layers.append(ReLU()) \n",
    "            self.sequence_conv_layers.append(MaxPooling2D(pool_size=3, stride=2))    #27*27\n",
    "            self.sequence_conv_layers.append(Conv2D(96, 256, kernel_size=5, stride=1, padding=2))  #27*27\n",
    "            self.sequence_conv_layers.append(ReLU())\n",
    "            self.sequence_conv_layers.append(MaxPooling2D(pool_size=3, stride=2))  #13*13\n",
    "            self.sequence_conv_layers.append(Conv2D(256, 384, kernel_size=3, stride=1, padding=1))  #13*13\n",
    "            self.sequence_conv_layers.append(ReLU())\n",
    "            self.sequence_conv_layers.append(Conv2D(384, 384, kernel_size=3, stride=1, padding=1))  #13*13\n",
    "            self.sequence_conv_layers.append(ReLU())\n",
    "            self.sequence_conv_layers.append(Conv2D(384, 256, kernel_size=3, stride=1, padding=1))  #13*13\n",
    "            self.sequence_conv_layers.append(ReLU())\n",
    "            self.sequence_conv_layers.append(MaxPooling2D(pool_size=3, stride=2))  #6*6\n",
    "            self.sequence_conv_layers.append(Flatten())\n",
    "\n",
    "        else:\n",
    "            for i in range(num_conv_layers):\n",
    "                print('输入第', i+1, '层卷积层后是否有池化层：\\n1.有\\n2.无')  \n",
    "                choice = int(input())\n",
    "                print('输入num_filters：')\n",
    "                num_filters = int(input())\n",
    "                print('输入kernel_size：')\n",
    "                kernel_size = int(input())\n",
    "                print('输入stride：')\n",
    "                stride = int(input())\n",
    "                print('输入padding：')\n",
    "                padding = int(input())\n",
    "                self.sequence_conv_layers.append(Conv2D(input_channels, num_filters, kernel_size, stride, padding))\n",
    "                \n",
    "                self.sequence_conv_layers.append(ReLU())\n",
    "                if choice == 1:\n",
    "                    self.sequence_conv_layers.append(MaxPooling2D(pool_size=2, stride=2))\n",
    "                input_channels = num_filters\n",
    "                if i == num_conv_layers-1:\n",
    "                    self.sequence_conv_layers.append(Flatten())\n",
    "\n",
    "        self.sequence_dense_layers = [] #全连接层列表\n",
    "        if self.num_dense_layers == -1:#Alexnet模型\n",
    "            \"\"\" self.dense1 = Dense(10*6*6, 100)\n",
    "            self.bn6 = BatchNorm(100)\n",
    "            self.relu6 = ReLU()\n",
    "            self.dropout1 = Dropout(0.5)\n",
    "            self.dense2 = Dense(100, 100)\n",
    "            self.bn7 = BatchNorm(100)\n",
    "            self.relu7 = ReLU()\n",
    "            self.dropout2 = Dropout(0.5)\n",
    "            self.dense3 = Dense(100, 10) \"\"\"\n",
    "\n",
    "            self.sequence_dense_layers.append(Dense(256*6*6, 4096))\n",
    "            self.sequence_dense_layers.append(BatchNorm(4096))\n",
    "            self.sequence_dense_layers.append(ReLU())\n",
    "            self.sequence_dense_layers.append(Dropout(0.2))\n",
    "            self.sequence_dense_layers.append(Dense(4096, 4096))\n",
    "            self.sequence_dense_layers.append(BatchNorm(4096))\n",
    "            self.sequence_dense_layers.append(ReLU())\n",
    "            self.sequence_dense_layers.append(Dropout(0.2))\n",
    "            self.sequence_dense_layers.append(Dense(4096, 10)) \n",
    "            \n",
    "\n",
    "        else:\n",
    "            num_units = 0\n",
    "            num_pre = 5*6*6\n",
    "            for i in range(num_dense_layers):\n",
    "                if i == num_dense_layers - 1:\n",
    "                    self.sequence_dense_layers.append(Dense(num_pre, 10))\n",
    "                    break\n",
    "                print('输入第', i+1, '层全连接层输出的神经元个数')  \n",
    "                num_units = int(input())\n",
    "                self.sequence_dense_layers.append(Dense(num_pre, num_units))\n",
    "                self.sequence_dense_layers.append(BatchNorm(num_units))\n",
    "                self.sequence_dense_layers.append(ReLU())\n",
    "                self.sequence_dense_layers.append(Dropout(0.2))\n",
    "                num_pre = num_units\n",
    "        \n",
    "            \n",
    "\n",
    "    def forward(self, inputs, is_training=True):\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        \"\"\" inputs = self.conv1.forward(inputs)\n",
    "        \n",
    "        inputs = self.relu1.forward(inputs)\n",
    "        inputs = self.maxpool1.forward(inputs)\n",
    "        inputs = self.conv2.forward(inputs)\n",
    "        \n",
    "        inputs = self.relu2.forward(inputs)\n",
    "        inputs = self.maxpool2.forward(inputs)\n",
    "        inputs = self.conv3.forward(inputs)\n",
    "        \n",
    "        inputs = self.relu3.forward(inputs)\n",
    "        inputs = self.conv4.forward(inputs)\n",
    "        \n",
    "        inputs = self.relu4.forward(inputs)\n",
    "        inputs = self.conv5.forward(inputs)\n",
    "        \n",
    "        inputs = self.relu5.forward(inputs)\n",
    "        inputs = self.maxpool3.forward(inputs)\n",
    "        inputs = self.flatten.forward(inputs)\n",
    "        inputs = self.dense1.forward(inputs)\n",
    "        inputs = self.bn6.forward(inputs, is_training)\n",
    "        inputs = self.relu6.forward(inputs)\n",
    "        inputs = self.dropout1.forward(inputs, is_training)\n",
    "        inputs = self.dense2.forward(inputs)\n",
    "        inputs = self.bn7.forward(inputs, is_training)\n",
    "        inputs = self.relu7.forward(inputs)\n",
    "        inputs = self.dropout2.forward(inputs, is_training)\n",
    "        inputs = self.dense3.forward(inputs) \"\"\"\n",
    "\n",
    "        for fun in self.sequence_conv_layers:\n",
    "            if isinstance(fun, Conv2D):\n",
    "                inputs = fun.forward(inputs)     \n",
    "            elif isinstance(fun, BatchNorm):\n",
    "                inputs = fun.forward(inputs, is_training)\n",
    "            elif isinstance(fun, ReLU):\n",
    "                inputs = fun.forward(inputs)\n",
    "            elif isinstance(fun, MaxPooling2D):\n",
    "                inputs = fun.forward(inputs)\n",
    "            elif isinstance(fun, Flatten):\n",
    "                inputs = fun.forward(inputs)\n",
    "                \n",
    "                \n",
    "        \n",
    "        for fun in self.sequence_dense_layers:\n",
    "            if isinstance(fun, Dense):\n",
    "                inputs = fun.forward(inputs)\n",
    "            elif isinstance(fun, BatchNorm):\n",
    "                inputs = fun.forward(inputs, is_training)\n",
    "            elif isinstance(fun, ReLU):\n",
    "                inputs = fun.forward(inputs)\n",
    "            elif isinstance(fun, Dropout):\n",
    "                inputs = fun.forward(inputs, is_training) \n",
    "\n",
    "        x = inputs\n",
    "        return x\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        \n",
    "        \n",
    "        for fun in reversed(self.sequence_dense_layers):\n",
    "            if isinstance(fun, Dense):\n",
    "                d_out = fun.backward(d_out, learning_rate)\n",
    "            elif isinstance(fun, BatchNorm):\n",
    "                d_out = fun.backward(d_out, learning_rate)   \n",
    "            elif isinstance(fun, ReLU):\n",
    "                d_out = fun.backward(d_out)\n",
    "            elif isinstance(fun, Dropout):\n",
    "                d_out = fun.backward(d_out)\n",
    "                   \n",
    "\n",
    "        for fun in reversed(self.sequence_conv_layers):\n",
    "            if isinstance(fun, Conv2D):\n",
    "                \n",
    "                d_out = fun.backward(d_out, learning_rate)\n",
    "            elif isinstance(fun, BatchNorm):\n",
    "                d_out = fun.backward(d_out, learning_rate)\n",
    "            elif isinstance(fun, ReLU):\n",
    "                d_out = fun.backward(d_out)\n",
    "            elif isinstance(fun, MaxPooling2D):\n",
    "                d_out = fun.backward(d_out)\n",
    "            elif isinstance(fun, Flatten):\n",
    "                d_out = fun.backward(d_out) \n",
    "\n",
    "        \"\"\" d_out = self.dense3.backward(d_out, learning_rate)\n",
    "        d_out = self.dropout2.backward(d_out)\n",
    "        d_out = self.relu7.backward(d_out)\n",
    "        d_out = self.bn7.backward(d_out, learning_rate)\n",
    "        d_out = self.dense2.backward(d_out, learning_rate)\n",
    "        d_out = self.dropout1.backward(d_out)\n",
    "        d_out = self.relu6.backward(d_out)\n",
    "        d_out = self.bn6.backward(d_out, learning_rate)\n",
    "        d_out = self.dense1.backward(d_out, learning_rate)\n",
    "        d_out = self.flatten.backward(d_out)\n",
    "        d_out = self.maxpool3.backward(d_out)\n",
    "        d_out = self.relu5.backward(d_out)\n",
    "        d_out = self.conv5.backward(d_out, learning_rate)\n",
    "        d_out = self.relu4.backward(d_out)\n",
    "        d_out = self.conv4.backward(d_out, learning_rate)\n",
    "        d_out = self.relu3.backward(d_out)\n",
    "        d_out = self.conv3.backward(d_out, learning_rate)\n",
    "        d_out = self.maxpool2.backward(d_out)\n",
    "        d_out = self.relu2.backward(d_out)\n",
    "        d_out = self.conv2.backward(d_out, learning_rate)\n",
    "        d_out = self.maxpool1.backward(d_out)\n",
    "        d_out = self.relu1.backward(d_out)\n",
    "        d_out = self.conv1.backward(d_out, learning_rate) \"\"\"\n",
    "        \n",
    "        \n",
    "        return d_out\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs, learning_rate):\n",
    "        loss_fn = CrossEntropyLoss()\n",
    "        step = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, X_train.shape[0], 16):\n",
    "                inputs = X_train[i:i+16]\n",
    "                labels = y_train[i:i+16]\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = loss_fn.forward(outputs, labels)\n",
    "\n",
    "\n",
    "                running_loss += loss\n",
    "                d_loss = loss_fn.backward()\n",
    "                self.backward(d_loss, learning_rate)\n",
    "\n",
    "                if step % 40 == 0:\n",
    "                    print(f'Step {step}, Loss: {loss:.4f}')\n",
    "                    \n",
    "                step += 1\n",
    "            print(f'Epoch {epoch + 1}, Loss: {running_loss / (X_train.shape[0] // 16):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构造模型：1.自定义 2.Alexnet\n",
      "请输入卷积层数量：\n",
      "请输入全连接层数量：\n",
      "输入第 1 层卷积层后是否有池化层：\n",
      "1.有\n",
      "2.无\n",
      "输入num_filters：\n",
      "输入kernel_size：\n",
      "输入stride：\n",
      "输入padding：\n",
      "输入第 2 层卷积层后是否有池化层：\n",
      "1.有\n",
      "2.无\n",
      "输入num_filters：\n",
      "输入kernel_size：\n",
      "输入stride：\n",
      "输入padding：\n",
      "输入第 1 层全连接层输出的神经元个数\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "划分完成\n",
      "Step 0, Loss: 4.4879\n",
      "Step 40, Loss: 3.7385\n",
      "Step 80, Loss: 2.9870\n",
      "Step 120, Loss: 2.3948\n",
      "Epoch 1, Loss: 3.3403\n",
      "Step 160, Loss: 1.9174\n",
      "Step 200, Loss: 1.5957\n",
      "Step 240, Loss: 1.3087\n",
      "Epoch 2, Loss: 1.7285\n",
      "Step 280, Loss: 1.0727\n",
      "Step 320, Loss: 0.8744\n",
      "Step 360, Loss: 0.7065\n",
      "Epoch 3, Loss: 0.9270\n"
     ]
    }
   ],
   "source": [
    "print('构造模型：1.自定义 2.Alexnet')\n",
    "model_type = int(input())\n",
    "if model_type == 1:\n",
    "    print('请输入卷积层数量：')\n",
    "    num_conv_layers = int(input())\n",
    "    print('请输入全连接层数量：')\n",
    "    num_dense_layers = int(input())\n",
    "else:\n",
    "    num_conv_layers = -1\n",
    "    num_dense_layers = -1\n",
    "\n",
    "model = CNN(num_conv_layers, num_dense_layers)\n",
    "\n",
    "# 定义数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "# 加载 CIFAR-10 训练和测试数据集\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "full_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "subset_indices = list(range(2000))\n",
    "trainset = Subset(full_trainset, subset_indices)\n",
    "testset = Subset(full_testset, subset_indices)\n",
    "print('划分完成')\n",
    "\n",
    "# 将数据转换为 NumPy 数组\n",
    "X_train = np.array([np.array(trainset[i][0]) for i in range(len(trainset))])\n",
    "y_train = np.array([trainset[i][1] for i in range(len(trainset))])\n",
    "\n",
    "X_test = np.array([np.array(testset[i][0]) for i in range(len(testset))])\n",
    "y_test = np.array([testset[i][1] for i in range(len(testset))])\n",
    "\n",
    "# 将标签转换为 one-hot 编码\n",
    "y_train_one_hot = np.zeros((y_train.size, 10))\n",
    "for i in range(y_train.size):\n",
    "    y_train_one_hot[i, y_train[i]] = 1\n",
    "\n",
    "y_test_one_hot = np.zeros((y_test.size, 10))\n",
    "for i in range(y_test.size):\n",
    "    y_test_one_hot[i, y_test[i]] = 1\n",
    "\n",
    "num_epochs = 3\n",
    "model.train(X_train, y_train_one_hot, num_epochs, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compared with pytorch\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1, Batch 1, Loss: 0.0231\n",
      "Epoch 1, Batch 41, Loss: 0.9080\n",
      "Epoch 1, Batch 81, Loss: 0.8452\n",
      "Epoch 1, Batch 121, Loss: 0.8174\n",
      "Epoch 1, Batch 161, Loss: 0.7897\n",
      "Epoch 1, Batch 201, Loss: 0.7794\n",
      "Epoch 1, Batch 241, Loss: 0.7482\n",
      "Epoch 1, Batch 281, Loss: 0.7554\n",
      "Epoch 1, Batch 321, Loss: 0.7507\n",
      "Epoch 1, Batch 361, Loss: 0.7463\n",
      "Epoch 1, Batch 401, Loss: 0.7012\n",
      "Epoch 1, Batch 441, Loss: 0.6946\n",
      "Epoch 1, Batch 481, Loss: 0.6964\n",
      "Epoch 1, Batch 521, Loss: 0.6668\n",
      "Epoch 1, Batch 561, Loss: 0.6932\n",
      "Epoch 1, Batch 601, Loss: 0.6801\n",
      "Epoch 1, Batch 641, Loss: 0.6587\n",
      "Epoch 1, Batch 681, Loss: 0.6706\n",
      "Epoch 1, Batch 721, Loss: 0.6698\n",
      "Epoch 1, Batch 761, Loss: 0.6497\n",
      "Epoch 1, Batch 801, Loss: 0.6439\n",
      "Epoch 1, Batch 841, Loss: 0.6609\n",
      "Epoch 1, Batch 881, Loss: 0.6296\n",
      "Epoch 1, Batch 921, Loss: 0.6304\n",
      "Epoch 1, Batch 961, Loss: 0.6697\n",
      "Epoch 1, Batch 1001, Loss: 0.6351\n",
      "Epoch 1, Batch 1041, Loss: 0.6224\n",
      "Epoch 1, Batch 1081, Loss: 0.6289\n",
      "Epoch 1, Batch 1121, Loss: 0.6237\n",
      "Epoch 1, Batch 1161, Loss: 0.5917\n",
      "Epoch 1, Batch 1201, Loss: 0.6291\n",
      "Epoch 1, Batch 1241, Loss: 0.6310\n",
      "Epoch 1, Batch 1281, Loss: 0.6318\n",
      "Epoch 1, Batch 1321, Loss: 0.6079\n",
      "Epoch 1, Batch 1361, Loss: 0.6105\n",
      "Epoch 1, Batch 1401, Loss: 0.6490\n",
      "Epoch 1, Batch 1441, Loss: 0.6101\n",
      "Epoch 1, Batch 1481, Loss: 0.6384\n",
      "Epoch 1, Batch 1521, Loss: 0.5939\n",
      "Epoch 1, Batch 1561, Loss: 0.6371\n",
      "Epoch 1, Batch 1601, Loss: 0.6392\n",
      "Epoch 1, Batch 1641, Loss: 0.6083\n",
      "Epoch 1, Batch 1681, Loss: 0.6049\n",
      "Epoch 1, Batch 1721, Loss: 0.6013\n",
      "Epoch 1, Batch 1761, Loss: 0.6259\n",
      "Epoch 1, Batch 1801, Loss: 0.6284\n",
      "Epoch 1, Batch 1841, Loss: 0.5935\n",
      "Epoch 1, Batch 1881, Loss: 0.6027\n",
      "Epoch 1, Batch 1921, Loss: 0.5933\n",
      "Epoch 1, Batch 1961, Loss: 0.6090\n",
      "Epoch 1, Batch 2001, Loss: 0.5777\n",
      "Epoch 1, Batch 2041, Loss: 0.6104\n",
      "Epoch 1, Batch 2081, Loss: 0.6132\n",
      "Epoch 1, Batch 2121, Loss: 0.5992\n",
      "Epoch 1, Batch 2161, Loss: 0.5682\n",
      "Epoch 1, Batch 2201, Loss: 0.6058\n",
      "Epoch 1, Batch 2241, Loss: 0.5890\n",
      "Epoch 1, Batch 2281, Loss: 0.5962\n",
      "Epoch 1, Batch 2321, Loss: 0.5880\n",
      "Epoch 1, Batch 2361, Loss: 0.5685\n",
      "Epoch 1, Batch 2401, Loss: 0.5733\n",
      "Epoch 1, Batch 2441, Loss: 0.5927\n",
      "Epoch 1, Batch 2481, Loss: 0.5699\n",
      "Epoch 1, Batch 2521, Loss: 0.5926\n",
      "Epoch 1, Batch 2561, Loss: 0.5888\n",
      "Epoch 1, Batch 2601, Loss: 0.5728\n",
      "Epoch 1, Batch 2641, Loss: 0.5671\n",
      "Epoch 1, Batch 2681, Loss: 0.5617\n",
      "Epoch 1, Batch 2721, Loss: 0.6056\n",
      "Epoch 1, Batch 2761, Loss: 0.5782\n",
      "Epoch 1, Batch 2801, Loss: 0.5734\n",
      "Epoch 1, Batch 2841, Loss: 0.5991\n",
      "Epoch 1, Batch 2881, Loss: 0.5971\n",
      "Epoch 1, Batch 2921, Loss: 0.5639\n",
      "Epoch 1, Batch 2961, Loss: 0.5700\n",
      "Epoch 1, Batch 3001, Loss: 0.5768\n",
      "Epoch 1, Batch 3041, Loss: 0.5693\n",
      "Epoch 1, Batch 3081, Loss: 0.5691\n",
      "Epoch 1, Batch 3121, Loss: 0.5541\n",
      "Epoch 2, Batch 1, Loss: 0.0105\n",
      "Epoch 2, Batch 41, Loss: 0.5650\n",
      "Epoch 2, Batch 81, Loss: 0.5273\n",
      "Epoch 2, Batch 121, Loss: 0.5289\n",
      "Epoch 2, Batch 161, Loss: 0.5402\n",
      "Epoch 2, Batch 201, Loss: 0.5438\n",
      "Epoch 2, Batch 241, Loss: 0.5588\n",
      "Epoch 2, Batch 281, Loss: 0.5874\n",
      "Epoch 2, Batch 321, Loss: 0.5391\n",
      "Epoch 2, Batch 361, Loss: 0.5486\n",
      "Epoch 2, Batch 401, Loss: 0.5759\n",
      "Epoch 2, Batch 441, Loss: 0.5954\n",
      "Epoch 2, Batch 481, Loss: 0.5466\n",
      "Epoch 2, Batch 521, Loss: 0.5541\n",
      "Epoch 2, Batch 561, Loss: 0.5703\n",
      "Epoch 2, Batch 601, Loss: 0.5808\n",
      "Epoch 2, Batch 641, Loss: 0.5230\n",
      "Epoch 2, Batch 681, Loss: 0.5555\n",
      "Epoch 2, Batch 721, Loss: 0.5642\n",
      "Epoch 2, Batch 761, Loss: 0.5054\n",
      "Epoch 2, Batch 801, Loss: 0.5623\n",
      "Epoch 2, Batch 841, Loss: 0.5360\n",
      "Epoch 2, Batch 881, Loss: 0.5752\n",
      "Epoch 2, Batch 921, Loss: 0.5577\n",
      "Epoch 2, Batch 961, Loss: 0.5517\n",
      "Epoch 2, Batch 1001, Loss: 0.5749\n",
      "Epoch 2, Batch 1041, Loss: 0.5333\n",
      "Epoch 2, Batch 1081, Loss: 0.5536\n",
      "Epoch 2, Batch 1121, Loss: 0.5334\n",
      "Epoch 2, Batch 1161, Loss: 0.5300\n",
      "Epoch 2, Batch 1201, Loss: 0.5469\n",
      "Epoch 2, Batch 1241, Loss: 0.5359\n",
      "Epoch 2, Batch 1281, Loss: 0.5283\n",
      "Epoch 2, Batch 1321, Loss: 0.5399\n",
      "Epoch 2, Batch 1361, Loss: 0.5427\n",
      "Epoch 2, Batch 1401, Loss: 0.5569\n",
      "Epoch 2, Batch 1441, Loss: 0.5192\n",
      "Epoch 2, Batch 1481, Loss: 0.5193\n",
      "Epoch 2, Batch 1521, Loss: 0.5263\n",
      "Epoch 2, Batch 1561, Loss: 0.5165\n",
      "Epoch 2, Batch 1601, Loss: 0.5248\n",
      "Epoch 2, Batch 1641, Loss: 0.5373\n",
      "Epoch 2, Batch 1681, Loss: 0.5583\n",
      "Epoch 2, Batch 1721, Loss: 0.4962\n",
      "Epoch 2, Batch 1761, Loss: 0.5637\n",
      "Epoch 2, Batch 1801, Loss: 0.5439\n",
      "Epoch 2, Batch 1841, Loss: 0.5505\n",
      "Epoch 2, Batch 1881, Loss: 0.5375\n",
      "Epoch 2, Batch 1921, Loss: 0.5628\n",
      "Epoch 2, Batch 1961, Loss: 0.5257\n",
      "Epoch 2, Batch 2001, Loss: 0.5533\n",
      "Epoch 2, Batch 2041, Loss: 0.5165\n",
      "Epoch 2, Batch 2081, Loss: 0.5481\n",
      "Epoch 2, Batch 2121, Loss: 0.5498\n",
      "Epoch 2, Batch 2161, Loss: 0.5122\n",
      "Epoch 2, Batch 2201, Loss: 0.5540\n",
      "Epoch 2, Batch 2241, Loss: 0.5680\n",
      "Epoch 2, Batch 2281, Loss: 0.5479\n",
      "Epoch 2, Batch 2321, Loss: 0.5223\n",
      "Epoch 2, Batch 2361, Loss: 0.5284\n",
      "Epoch 2, Batch 2401, Loss: 0.4914\n",
      "Epoch 2, Batch 2441, Loss: 0.5103\n",
      "Epoch 2, Batch 2481, Loss: 0.5140\n",
      "Epoch 2, Batch 2521, Loss: 0.4995\n",
      "Epoch 2, Batch 2561, Loss: 0.5251\n",
      "Epoch 2, Batch 2601, Loss: 0.5484\n",
      "Epoch 2, Batch 2641, Loss: 0.5238\n",
      "Epoch 2, Batch 2681, Loss: 0.5229\n",
      "Epoch 2, Batch 2721, Loss: 0.5284\n",
      "Epoch 2, Batch 2761, Loss: 0.5175\n",
      "Epoch 2, Batch 2801, Loss: 0.5182\n",
      "Epoch 2, Batch 2841, Loss: 0.5262\n",
      "Epoch 2, Batch 2881, Loss: 0.5132\n",
      "Epoch 2, Batch 2921, Loss: 0.5504\n",
      "Epoch 2, Batch 2961, Loss: 0.5072\n",
      "Epoch 2, Batch 3001, Loss: 0.5410\n",
      "Epoch 2, Batch 3041, Loss: 0.5187\n",
      "Epoch 2, Batch 3081, Loss: 0.5563\n",
      "Epoch 2, Batch 3121, Loss: 0.5359\n",
      "Epoch 3, Batch 1, Loss: 0.0090\n",
      "Epoch 3, Batch 41, Loss: 0.5325\n",
      "Epoch 3, Batch 81, Loss: 0.4803\n",
      "Epoch 3, Batch 121, Loss: 0.5177\n",
      "Epoch 3, Batch 161, Loss: 0.5077\n",
      "Epoch 3, Batch 201, Loss: 0.5000\n",
      "Epoch 3, Batch 241, Loss: 0.5023\n",
      "Epoch 3, Batch 281, Loss: 0.4960\n",
      "Epoch 3, Batch 321, Loss: 0.4966\n",
      "Epoch 3, Batch 361, Loss: 0.5168\n",
      "Epoch 3, Batch 401, Loss: 0.4922\n",
      "Epoch 3, Batch 441, Loss: 0.4988\n",
      "Epoch 3, Batch 481, Loss: 0.5284\n",
      "Epoch 3, Batch 521, Loss: 0.5019\n",
      "Epoch 3, Batch 561, Loss: 0.5168\n",
      "Epoch 3, Batch 601, Loss: 0.5048\n",
      "Epoch 3, Batch 641, Loss: 0.5085\n",
      "Epoch 3, Batch 681, Loss: 0.5019\n",
      "Epoch 3, Batch 721, Loss: 0.4958\n",
      "Epoch 3, Batch 761, Loss: 0.5277\n",
      "Epoch 3, Batch 801, Loss: 0.5025\n",
      "Epoch 3, Batch 841, Loss: 0.5030\n",
      "Epoch 3, Batch 881, Loss: 0.5082\n",
      "Epoch 3, Batch 921, Loss: 0.5278\n",
      "Epoch 3, Batch 961, Loss: 0.4798\n",
      "Epoch 3, Batch 1001, Loss: 0.5080\n",
      "Epoch 3, Batch 1041, Loss: 0.5063\n",
      "Epoch 3, Batch 1081, Loss: 0.4682\n",
      "Epoch 3, Batch 1121, Loss: 0.4912\n",
      "Epoch 3, Batch 1161, Loss: 0.5409\n",
      "Epoch 3, Batch 1201, Loss: 0.5035\n",
      "Epoch 3, Batch 1241, Loss: 0.4898\n",
      "Epoch 3, Batch 1281, Loss: 0.4701\n",
      "Epoch 3, Batch 1321, Loss: 0.4976\n",
      "Epoch 3, Batch 1361, Loss: 0.5077\n",
      "Epoch 3, Batch 1401, Loss: 0.4904\n",
      "Epoch 3, Batch 1441, Loss: 0.5072\n",
      "Epoch 3, Batch 1481, Loss: 0.5260\n",
      "Epoch 3, Batch 1521, Loss: 0.4866\n",
      "Epoch 3, Batch 1561, Loss: 0.4817\n",
      "Epoch 3, Batch 1601, Loss: 0.5021\n",
      "Epoch 3, Batch 1641, Loss: 0.4679\n",
      "Epoch 3, Batch 1681, Loss: 0.4762\n",
      "Epoch 3, Batch 1721, Loss: 0.5094\n",
      "Epoch 3, Batch 1761, Loss: 0.5263\n",
      "Epoch 3, Batch 1801, Loss: 0.4903\n",
      "Epoch 3, Batch 1841, Loss: 0.4927\n",
      "Epoch 3, Batch 1881, Loss: 0.5134\n",
      "Epoch 3, Batch 1921, Loss: 0.4708\n",
      "Epoch 3, Batch 1961, Loss: 0.4960\n",
      "Epoch 3, Batch 2001, Loss: 0.4456\n",
      "Epoch 3, Batch 2041, Loss: 0.4928\n",
      "Epoch 3, Batch 2081, Loss: 0.4958\n",
      "Epoch 3, Batch 2121, Loss: 0.4960\n",
      "Epoch 3, Batch 2161, Loss: 0.5063\n",
      "Epoch 3, Batch 2201, Loss: 0.5084\n",
      "Epoch 3, Batch 2241, Loss: 0.5336\n",
      "Epoch 3, Batch 2281, Loss: 0.5204\n",
      "Epoch 3, Batch 2321, Loss: 0.4807\n",
      "Epoch 3, Batch 2361, Loss: 0.4825\n",
      "Epoch 3, Batch 2401, Loss: 0.4804\n",
      "Epoch 3, Batch 2441, Loss: 0.4985\n",
      "Epoch 3, Batch 2481, Loss: 0.5148\n",
      "Epoch 3, Batch 2521, Loss: 0.4915\n",
      "Epoch 3, Batch 2561, Loss: 0.4787\n",
      "Epoch 3, Batch 2601, Loss: 0.4869\n",
      "Epoch 3, Batch 2641, Loss: 0.5014\n",
      "Epoch 3, Batch 2681, Loss: 0.5038\n",
      "Epoch 3, Batch 2721, Loss: 0.4907\n",
      "Epoch 3, Batch 2761, Loss: 0.4789\n",
      "Epoch 3, Batch 2801, Loss: 0.4858\n",
      "Epoch 3, Batch 2841, Loss: 0.4896\n",
      "Epoch 3, Batch 2881, Loss: 0.4779\n",
      "Epoch 3, Batch 2921, Loss: 0.4986\n",
      "Epoch 3, Batch 2961, Loss: 0.5037\n",
      "Epoch 3, Batch 3001, Loss: 0.5059\n",
      "Epoch 3, Batch 3041, Loss: 0.5068\n",
      "Epoch 3, Batch 3081, Loss: 0.5005\n",
      "Epoch 3, Batch 3121, Loss: 0.4953\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print('compared with pytorch')\n",
    "import torch.optim as optim\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "# 定义卷积神经网络\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=10, stride=2, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=5, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(5 * 6 * 6, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # 展平层\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "cnn = SimpleCNN()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            # 梯度置零\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = cnn(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 打印统计信息\n",
    "            running_loss += loss.item()\n",
    "            if i % 40 == 0:    \n",
    "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
