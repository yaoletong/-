{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fd3fde9d0c24ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:46:07.471671200Z",
     "start_time": "2024-06-23T02:46:03.504061600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa512084dacffdd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:46:22.665592100Z",
     "start_time": "2024-06-23T02:46:22.638466700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_corpus(file_path):\n",
    "    \"\"\"加载数据集，返回英文和中文的句子列表。\"\"\"\n",
    "    english_sentences = []\n",
    "    chinese_sentences = []\n",
    "    with open(file_path, 'r', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            english, chinese = line.strip().split('\\t')\n",
    "            english_sentences.append(['BOS'] + nltk.word_tokenize(english.lower()) + ['EOS'])\n",
    "            chinese_sentences.append(['BOS'] + list(chinese) + ['EOS'])\n",
    "    return english_sentences, chinese_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6335569754fae354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:46:28.481932200Z",
     "start_time": "2024-06-23T02:46:26.376499500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', 'she', 'put', 'the', 'magazine', 'on', 'the', 'table', '.', 'EOS'], ['BOS', 'hey', ',', 'what', 'are', 'you', 'doing', 'here', '?', 'EOS']]\n",
      "[['BOS', '她', '把', '雜', '誌', '放', '在', '桌', '上', '。', 'EOS'], ['BOS', '嘿', '，', '你', '在', '這', '做', '什', '麼', '？', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "# 文件路径\n",
    "train_data_path = 'nmt/en-cn/train.txt'\n",
    "dev_data_path = 'nmt/en-cn/dev.txt'\n",
    "\n",
    "# 加载数据\n",
    "train_english_sentences, train_chinese_sentences = load_corpus(train_data_path)\n",
    "dev_english_sentences, dev_chinese_sentences = load_corpus(dev_data_path)\n",
    "\n",
    "print(dev_english_sentences[:2])\n",
    "print(dev_chinese_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7bb9de3eedcae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:46:32.109028200Z",
     "start_time": "2024-06-23T02:46:31.952902900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 982, 2028, 8, 4, 3]\n",
      "['BOS', '祝', '贺', '你', '。', 'EOS']\n",
      "['BOS', 'congratulations', '!', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "# 特殊标记索引\n",
    "UNK_INDEX = 0\n",
    "PAD_INDEX = 1\n",
    "\n",
    "\n",
    "def create_vocab(sentences, max_words=50000):\n",
    "    \"\"\"创建词汇表，返回词汇到索引的映射和总词汇量。\"\"\"\n",
    "    word_counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_counter[word] += 1\n",
    "\n",
    "    most_common_words = word_counter.most_common(max_words)\n",
    "    total_vocab_size = len(most_common_words) + 2  # 加上PAD和UNK\n",
    "\n",
    "    vocab = {word: idx + 2 for idx, (word, _) in enumerate(most_common_words)}\n",
    "    vocab['UNK'] = UNK_INDEX\n",
    "    vocab['PAD'] = PAD_INDEX\n",
    "\n",
    "    return vocab, total_vocab_size\n",
    "\n",
    "\n",
    "# 构建词汇表\n",
    "english_vocab, english_vocab_size = create_vocab(train_english_sentences)\n",
    "chinese_vocab, chinese_vocab_size = create_vocab(train_chinese_sentences)\n",
    "\n",
    "# 反向词典映射\n",
    "reverse_english_vocab = {index: word for word, index in english_vocab.items()}\n",
    "reverse_chinese_vocab = {index: word for word, index in chinese_vocab.items()}\n",
    "\n",
    "\n",
    "def encode_sentences(english_sentences, chinese_sentences, english_vocab, chinese_vocab, sort_by_len=True):\n",
    "    \"\"\"将句子编码为索引，并按长度排序。\"\"\"\n",
    "    english_encoded = [[english_vocab.get(word, UNK_INDEX) for word in sentence] for sentence in english_sentences]\n",
    "    chinese_encoded = [[chinese_vocab.get(word, UNK_INDEX) for word in sentence] for sentence in chinese_sentences]\n",
    "\n",
    "    if sort_by_len:\n",
    "        sorted_indices = sorted(range(len(english_encoded)), key=lambda idx: len(english_encoded[idx]))\n",
    "        english_encoded = [english_encoded[i] for i in sorted_indices]\n",
    "        chinese_encoded = [chinese_encoded[i] for i in sorted_indices]\n",
    "\n",
    "    return english_encoded, chinese_encoded\n",
    "\n",
    "\n",
    "# 对训练和开发集进行编码\n",
    "encoded_train_english, encoded_train_chinese = encode_sentences(train_english_sentences, train_chinese_sentences,\n",
    "                                                                english_vocab, chinese_vocab)\n",
    "encoded_dev_english, encoded_dev_chinese = encode_sentences(dev_english_sentences, dev_chinese_sentences, english_vocab,\n",
    "                                                            chinese_vocab)\n",
    "\n",
    "print(encoded_train_chinese[2])\n",
    "print([reverse_chinese_vocab[i] for i in encoded_train_chinese[2]])\n",
    "print([reverse_english_vocab[i] for i in encoded_train_english[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cecd30a58eb1047",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:46:37.239386700Z",
     "start_time": "2024-06-23T02:46:37.181241500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_minibatches(data_size, batch_size, shuffle=True):\n",
    "    \"\"\"生成小批次的索引列表。\"\"\"\n",
    "    index_list = np.arange(0, data_size, batch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_list)\n",
    "    minibatches = [np.arange(idx, min(idx + batch_size, data_size)) for idx in index_list]\n",
    "    return minibatches\n",
    "\n",
    "\n",
    "def pad_sequences(sequences):\n",
    "    \"\"\"对批次中的序列进行填充，并返回填充后的序列和原始长度。\"\"\"\n",
    "    sequence_lengths = [len(seq) for seq in sequences]\n",
    "    num_samples = len(sequences)\n",
    "    max_seq_len = max(sequence_lengths)\n",
    "\n",
    "    padded_sequences = np.zeros((num_samples, max_seq_len)).astype('int32')\n",
    "    for idx, seq in enumerate(sequences):\n",
    "        padded_sequences[idx, :len(seq)] = seq\n",
    "\n",
    "    return padded_sequences, np.array(sequence_lengths).astype('int32')\n",
    "\n",
    "\n",
    "def prepare_batches(english_encoded, chinese_encoded, batch_size):\n",
    "    \"\"\"生成小批次的数据，包含填充后的序列和长度信息。\"\"\"\n",
    "    minibatches = generate_minibatches(len(english_encoded), batch_size)\n",
    "    batches = []\n",
    "    for batch_indices in minibatches:\n",
    "        batch_english = [english_encoded[idx] for idx in batch_indices]\n",
    "        batch_chinese = [chinese_encoded[idx] for idx in batch_indices]\n",
    "        padded_english, english_lengths = pad_sequences(batch_english)\n",
    "        padded_chinese, chinese_lengths = pad_sequences(batch_chinese)\n",
    "        batches.append((padded_english, english_lengths, padded_chinese, chinese_lengths))\n",
    "    return batches\n",
    "\n",
    "\n",
    "# 准备训练和验证数据\n",
    "batch_size = 64\n",
    "train_batches = prepare_batches(encoded_train_english, encoded_train_chinese, batch_size)\n",
    "dev_batches = prepare_batches(encoded_dev_english, encoded_dev_chinese, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83c5ea70a483176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:46:40.736975600Z",
     "start_time": "2024-06-23T02:46:40.678956400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, mask):\n",
    "        inputs = inputs.contiguous().view(-1, inputs.size(2))\n",
    "        targets = targets.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        log_probs = -inputs.gather(1, targets) * mask\n",
    "        loss = torch.sum(log_probs) / torch.sum(mask)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, rnn_type='GRU', dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn_type = rnn_type\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid rnn_type. Must be 'lstm' or 'gru'.\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(),\n",
    "                                                            batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hid = (hid[0][:, original_idx.long()].contiguous(), hid[1][:, original_idx.long()].contiguous())\n",
    "            hid = torch.cat([hid[0][-2], hid[0][-1]], dim=1)  # 将最后一层的hid的双向拼接\n",
    "        else:\n",
    "            hid = hid[:, original_idx.long()].contiguous()\n",
    "            hid = torch.cat([hid[-2], hid[-1]], dim=1)  # 将最后一层的hid的双向拼接\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "        return out, hid\n",
    "\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size, attn_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.attn_size = attn_size\n",
    "        self.linear_enc = nn.Linear(enc_hidden_size * 2, attn_size, bias=False)\n",
    "        self.linear_dec = nn.Linear(dec_hidden_size, attn_size, bias=False)\n",
    "        self.v = nn.Parameter(torch.rand(attn_size))\n",
    "\n",
    "    def forward(self, output, context, mask):\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        enc_transform = self.linear_enc(context.view(batch_size * input_len, -1)).view(batch_size, input_len, -1)\n",
    "        dec_transform = self.linear_dec(output.view(batch_size * output_len, -1)).view(batch_size, output_len, -1)\n",
    "        attn_scores = torch.tanh(enc_transform[:, None, :, :] + dec_transform[:, :, None, :])\n",
    "        attn_scores = torch.sum(self.v * attn_scores, dim=-1)\n",
    "        attn_scores.data.masked_fill(mask.bool(), -1e6)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        context = torch.bmm(attn_weights, context)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        # enc_hidden_size跟Encoder的一样\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.linear_in = nn.Linear(enc_hidden_size * 2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size * 2 + dec_hidden_size, dec_hidden_size)\n",
    "\n",
    "    def forward(self, output, context, mask):\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        context_in = self.linear_in(context.view(batch_size * input_len, -1)).view(batch_size, input_len, -1)\n",
    "        attn = torch.bmm(output, context_in.transpose(1, 2))\n",
    "        attn.data.masked_fill(mask.bool(), -1e6)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        context = torch.bmm(attn, context)\n",
    "        output = torch.cat((context, output), dim=2)\n",
    "        output = output.view(batch_size * output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, rnn_type='GRU',\n",
    "                 attn_type='luong', attn_size=50, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn_type = rnn_type\n",
    "        if attn_type == 'luong':\n",
    "            self.attention = LuongAttention(enc_hidden_size, dec_hidden_size)\n",
    "        elif attn_type == 'bahdanau':\n",
    "            self.attention = BahdanauAttention(enc_hidden_size, dec_hidden_size, attn_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid attn_type. Must be 'luong' or 'bahdanau'.\")\n",
    "\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embed_size, dec_hidden_size, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embed_size, dec_hidden_size, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid rnn_type. Must be 'lstm' or 'gru'.\")\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=device)[None, :] < y_len[:, None]\n",
    "        mask = (~ x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, encoder_out, x_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        if isinstance(hid, tuple):\n",
    "            hid = (hid[0][:, sorted_idx.long()], hid[1][:, sorted_idx.long()])\n",
    "        else:\n",
    "            hid = hid[:, sorted_idx.long()]\n",
    "        y_sorted = self.dropout(self.embed(y_sorted))\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        # out, hid = self.rnn(packed_seq, hid)\n",
    "        if self.rnn_type == 'GRU':\n",
    "            out, hid = self.rnn(packed_seq, hid)\n",
    "        else:\n",
    "            out, hid = self.rnn(packed_seq, (hid, torch.zeros_like(hid)))\n",
    "            # out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        if isinstance(hid, tuple):\n",
    "            hid = (hid[0][:, original_idx.long()].contiguous(), hid[1][:, original_idx.long()].contiguous())\n",
    "        else:\n",
    "            hid = hid[:, original_idx.long()].contiguous()\n",
    "        mask = self.create_mask(y_lengths, x_lengths)\n",
    "        output, attn = self.attention(output_seq, encoder_out, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        return output, hid, attn\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, rnn_type='GRU', attn_type='luong'):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        if rnn_type == 'GRU':\n",
    "            self.encoder = Encoder(vocab_size=english_vocab_size,\n",
    "                                   embed_size=embed_size,\n",
    "                                   enc_hidden_size=hidden_size,\n",
    "                                   dec_hidden_size=hidden_size,\n",
    "                                   dropout=dropout)\n",
    "            if attn_type == 'luong':\n",
    "                self.decoder = Decoder(vocab_size=chinese_vocab_size,\n",
    "                                       embed_size=embed_size,\n",
    "                                       enc_hidden_size=hidden_size,\n",
    "                                       dec_hidden_size=hidden_size,\n",
    "                                       dropout=dropout)\n",
    "            elif attn_type == 'bahdanau':\n",
    "                self.decoder = Decoder(vocab_size=chinese_vocab_size,\n",
    "                                       embed_size=embed_size,\n",
    "                                       enc_hidden_size=hidden_size,\n",
    "                                       dec_hidden_size=hidden_size,\n",
    "                                       dropout=dropout,\n",
    "                                       attn_type='bahdanau')\n",
    "            else:\n",
    "                raise ValueError(\"Invalid attn_type. Must be 'luong' or 'bahdanau'.\")\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.encoder = Encoder(vocab_size=english_vocab_size,\n",
    "                                   embed_size=embed_size,\n",
    "                                   enc_hidden_size=hidden_size,\n",
    "                                   dec_hidden_size=hidden_size,\n",
    "                                   dropout=dropout,\n",
    "                                   rnn_type='LSTM')\n",
    "            if attn_type == 'luong':\n",
    "                self.decoder = Decoder(vocab_size=chinese_vocab_size,\n",
    "                                       embed_size=embed_size,\n",
    "                                       enc_hidden_size=hidden_size,\n",
    "                                       dec_hidden_size=hidden_size,\n",
    "                                       dropout=dropout,\n",
    "                                       rnn_type='LSTM')\n",
    "            elif attn_type == 'bahdanau':\n",
    "                self.decoder = Decoder(vocab_size=chinese_vocab_size,\n",
    "                                       embed_size=embed_size,\n",
    "                                       enc_hidden_size=hidden_size,\n",
    "                                       dec_hidden_size=hidden_size,\n",
    "                                       dropout=dropout,\n",
    "                                       rnn_type='LSTM',\n",
    "                                       attn_type='bahdanau')\n",
    "            else:\n",
    "                raise ValueError(\"Invalid attn_type. Must be 'luong' or 'bahdanau'.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid rnn_type. Must be 'lstm' or 'gru'.\")\n",
    "\n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(encoder_out=encoder_out,\n",
    "                                         x_lengths=x_lengths,\n",
    "                                         y=y,\n",
    "                                         y_lengths=y_lengths,\n",
    "                                         hid=hid)\n",
    "        return output, attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(encoder_out,\n",
    "                                             x_lengths,\n",
    "                                             y,\n",
    "                                             torch.ones(batch_size).long().to(y.device),\n",
    "                                             hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626a7183181e298f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:46:47.012142700Z",
     "start_time": "2024-06-23T02:46:46.995450Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 训练模式\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (batch_input_sequences, batch_input_lengths, batch_target_sequences, batch_target_lengths) in enumerate(\n",
    "                data):\n",
    "            batch_input_sequences = torch.from_numpy(batch_input_sequences).to(device).long()\n",
    "            batch_input_lengths = torch.from_numpy(batch_input_lengths).to(device).long()\n",
    "\n",
    "            batch_input_for_decoder = torch.from_numpy(batch_target_sequences[:, :-1]).to(device).long()  # EOS之前\n",
    "            batch_expected_output = torch.from_numpy(batch_target_sequences[:, 1:]).to(device).long()  # BOS之后\n",
    "\n",
    "            batch_target_lengths = torch.from_numpy(batch_target_lengths - 1).to(device).long()\n",
    "            batch_target_lengths[batch_target_lengths <= 0] = 1\n",
    "            # print(f\"batch_input_sequences shape: {mb_x.shape}, batch_input_lengths: {mb_x_len}\")\n",
    "            # print(f\"batch_input_for_decoder shape: {batch_input_for_decoder.shape}, mb_y_len: {mb_y_len}\")\n",
    "            batch_predictions, attention_weights = model(batch_input_sequences, batch_input_lengths,\n",
    "                                                         batch_input_for_decoder, batch_target_lengths)\n",
    "            output_mask = torch.arange(batch_target_lengths.max().item(), device=device)[None,\n",
    "                          :] < batch_target_lengths[:, None]\n",
    "            output_mask = output_mask.float()\n",
    "\n",
    "            loss = loss_fn(batch_predictions, batch_expected_output, output_mask)\n",
    "\n",
    "            num_words = torch.sum(batch_target_lengths).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "\n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch: \", epoch, 'iteration', it, 'loss:', loss.item())\n",
    "\n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss / total_num_words)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_batches)\n",
    "\n",
    "    torch.save(model.state_dict(), 'translate_model.pt')\n",
    "\n",
    "\n",
    "def count_ngrams(sequence, n):\n",
    "    return Counter([tuple(sequence[i:i + n]) for i in range(len(sequence) - n + 1)])\n",
    "\n",
    "\n",
    "def compute_bleu_score(reference, hypothesis, max_n=4):\n",
    "    weights = [1 / max_n] * max_n  # 均匀分配权重\n",
    "    p_ns = []\n",
    "    for i in range(1, max_n + 1):\n",
    "        ref_ngrams = count_ngrams(reference, i)\n",
    "        hyp_ngrams = count_ngrams(hypothesis, i)\n",
    "        overlap = sum((hyp_ngrams & ref_ngrams).values())\n",
    "        total = max(1, sum(hyp_ngrams.values()))\n",
    "        p_ns.append(overlap / total)\n",
    "\n",
    "    # 几何平均\n",
    "    s = (weights[i] * np.log(p_ns[i]) for i in range(max_n) if p_ns[i] > 0)\n",
    "    geo_mean = np.exp(np.sum(list(s)))\n",
    "\n",
    "    # 惩罚因子 BP\n",
    "    bp = np.exp(1 - len(reference) / len(hypothesis)) if len(hypothesis) < len(reference) else 1\n",
    "\n",
    "    return bp * geo_mean\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    bleu_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (batch_input_sequences, batch_input_lengths, batch_target_sequences, batch_target_lengths) in enumerate(\n",
    "                data):\n",
    "            batch_input_sequences = torch.from_numpy(batch_input_sequences).to(device).long()\n",
    "            batch_input_lengths = torch.from_numpy(batch_input_lengths).to(device).long()\n",
    "            batch_input_for_decoder = torch.from_numpy(batch_target_sequences[:, :-1]).to(device).long()\n",
    "            batch_expected_output = torch.from_numpy(batch_target_sequences[:, 1:]).to(device).long()\n",
    "            batch_target_lengths = torch.from_numpy(batch_target_lengths - 1).to(device).long()\n",
    "            batch_target_lengths[batch_target_lengths <= 0] = 1\n",
    "\n",
    "            batch_predictions, attention_weights = model(batch_input_sequences, batch_input_lengths,\n",
    "                                                         batch_input_for_decoder, batch_target_lengths)\n",
    "\n",
    "            output_mask = torch.arange(batch_target_lengths.max().item(), device=device)[None,\n",
    "                          :] < batch_target_lengths[:, None]\n",
    "            output_mask = output_mask.float()\n",
    "\n",
    "            loss = loss_fn(batch_predictions, batch_expected_output, output_mask)\n",
    "\n",
    "            num_words = torch.sum(batch_target_lengths).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            # 生成翻译结果\n",
    "            _, predicted_indices = torch.max(batch_predictions, dim=-1)\n",
    "            predicted_indices = predicted_indices.cpu().numpy()\n",
    "\n",
    "            for ref, hyp in zip(batch_target_sequences[:, 1:], predicted_indices):\n",
    "                ref = ref[:np.where(ref == 0)[0][0]] if np.any(ref == 0) else ref  # 找到EOS位置\n",
    "                hyp = hyp[:np.where(hyp == 0)[0][0]] if np.any(hyp == 0) else hyp  # 找到EOS位置\n",
    "\n",
    "                if len(ref) == 0 or len(hyp) == 0:\n",
    "                    continue  # 跳过空的引用或预测\n",
    "\n",
    "                bleu_score = compute_bleu_score(ref, hyp)\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "            if len(bleu_scores) > 0:\n",
    "                avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "            else:\n",
    "                avg_bleu = 0.0\n",
    "\n",
    "    print(\"Evaluation loss\", total_loss / total_num_words)\n",
    "    print(\"BLEU score\", avg_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8b288c5947f5f50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:46:51.468409900Z",
     "start_time": "2024-06-23T02:46:51.413368500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def translate_dev(model, i):\n",
    "    en_sent = \" \".join([reverse_english_vocab[w] for w in encoded_dev_english[i]])  # 原来的英文\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([reverse_chinese_vocab[w] for w in encoded_dev_chinese[i]])  # 原来的中文\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    # 一条句子\n",
    "    mb_x = torch.from_numpy(np.array(encoded_dev_english[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(encoded_dev_english[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[chinese_vocab[\"BOS\"]]]).long().to(device)  # shape:[1,1], [[2]]\n",
    "\n",
    "    # y_lengths: [[2]], 一个句子\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)  # [1, 10]\n",
    "    # 映射成中文\n",
    "    translation = [reverse_chinese_vocab[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38156ef651ea19a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T02:56:01.799944600Z",
     "start_time": "2024-06-23T02:49:24.953135500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 iteration 0 loss: 8.074766159057617\n",
      "Epoch:  0 iteration 100 loss: 5.226406574249268\n",
      "Epoch:  0 iteration 200 loss: 5.1871466636657715\n",
      "Epoch 0 Training loss 5.490964172596266\n",
      "Evaluation loss 5.056133363791218\n",
      "BLEU score 0.3667682360922284\n",
      "Epoch:  1 iteration 0 loss: 5.145294666290283\n",
      "Epoch:  1 iteration 100 loss: 4.796230792999268\n",
      "Epoch:  1 iteration 200 loss: 4.80791711807251\n",
      "Epoch 1 Training loss 4.886493459473082\n",
      "Epoch:  2 iteration 0 loss: 4.740213871002197\n",
      "Epoch:  2 iteration 100 loss: 4.375669479370117\n",
      "Epoch:  2 iteration 200 loss: 4.453046798706055\n",
      "Epoch 2 Training loss 4.4746204288048546\n",
      "Epoch:  3 iteration 0 loss: 4.315225601196289\n",
      "Epoch:  3 iteration 100 loss: 3.9947047233581543\n",
      "Epoch:  3 iteration 200 loss: 4.143964767456055\n",
      "Epoch 3 Training loss 4.12537051358143\n",
      "Epoch:  4 iteration 0 loss: 4.000865936279297\n",
      "Epoch:  4 iteration 100 loss: 3.7372665405273438\n",
      "Epoch:  4 iteration 200 loss: 3.9123713970184326\n",
      "Epoch 4 Training loss 3.8521657546065753\n",
      "Epoch:  5 iteration 0 loss: 3.726984977722168\n",
      "Epoch:  5 iteration 100 loss: 3.544987201690674\n",
      "Epoch:  5 iteration 200 loss: 3.737780809402466\n",
      "Epoch 5 Training loss 3.635761543577851\n",
      "Evaluation loss 3.656507157138008\n",
      "BLEU score 0.3741729359409015\n",
      "Epoch:  6 iteration 0 loss: 3.542065382003784\n",
      "Epoch:  6 iteration 100 loss: 3.4086785316467285\n",
      "Epoch:  6 iteration 200 loss: 3.5628252029418945\n",
      "Epoch 6 Training loss 3.4587158155625795\n",
      "Epoch:  7 iteration 0 loss: 3.389385938644409\n",
      "Epoch:  7 iteration 100 loss: 3.226639986038208\n",
      "Epoch:  7 iteration 200 loss: 3.4178130626678467\n",
      "Epoch 7 Training loss 3.3079481278491047\n",
      "Epoch:  8 iteration 0 loss: 3.223554849624634\n",
      "Epoch:  8 iteration 100 loss: 3.1349096298217773\n",
      "Epoch:  8 iteration 200 loss: 3.2865347862243652\n",
      "Epoch 8 Training loss 3.1769573143874674\n",
      "Epoch:  9 iteration 0 loss: 3.084796905517578\n",
      "Epoch:  9 iteration 100 loss: 3.0480709075927734\n",
      "Epoch:  9 iteration 200 loss: 3.160428047180176\n",
      "Epoch 9 Training loss 3.062286108178845\n",
      "Epoch:  10 iteration 0 loss: 3.0045738220214844\n",
      "Epoch:  10 iteration 100 loss: 2.926616668701172\n",
      "Epoch:  10 iteration 200 loss: 3.045844554901123\n",
      "Epoch 10 Training loss 2.963041412120171\n",
      "Evaluation loss 3.24930143020471\n",
      "BLEU score 0.3658353119287786\n",
      "Epoch:  11 iteration 0 loss: 2.9218575954437256\n",
      "Epoch:  11 iteration 100 loss: 2.8360562324523926\n",
      "Epoch:  11 iteration 200 loss: 2.980931520462036\n",
      "Epoch 11 Training loss 2.868057918864137\n",
      "Epoch:  12 iteration 0 loss: 2.8087995052337646\n",
      "Epoch:  12 iteration 100 loss: 2.768768072128296\n",
      "Epoch:  12 iteration 200 loss: 2.8676252365112305\n",
      "Epoch 12 Training loss 2.787194624712798\n",
      "Epoch:  13 iteration 0 loss: 2.7412078380584717\n",
      "Epoch:  13 iteration 100 loss: 2.69848370552063\n",
      "Epoch:  13 iteration 200 loss: 2.8045883178710938\n",
      "Epoch 13 Training loss 2.7092206149423967\n",
      "Epoch:  14 iteration 0 loss: 2.6944804191589355\n",
      "Epoch:  14 iteration 100 loss: 2.609046459197998\n",
      "Epoch:  14 iteration 200 loss: 2.725785493850708\n",
      "Epoch 14 Training loss 2.636522867381026\n",
      "Epoch:  15 iteration 0 loss: 2.5592753887176514\n",
      "Epoch:  15 iteration 100 loss: 2.524529457092285\n",
      "Epoch:  15 iteration 200 loss: 2.623023748397827\n",
      "Epoch 15 Training loss 2.5708914625388286\n",
      "Evaluation loss 3.074767152133728\n",
      "BLEU score 0.3622296242316604\n",
      "Epoch:  16 iteration 0 loss: 2.5636003017425537\n",
      "Epoch:  16 iteration 100 loss: 2.4786949157714844\n",
      "Epoch:  16 iteration 200 loss: 2.567852258682251\n",
      "Epoch 16 Training loss 2.5069424251808696\n",
      "Epoch:  17 iteration 0 loss: 2.468029022216797\n",
      "Epoch:  17 iteration 100 loss: 2.4326603412628174\n",
      "Epoch:  17 iteration 200 loss: 2.5352983474731445\n",
      "Epoch 17 Training loss 2.4533943189731833\n",
      "Epoch:  18 iteration 0 loss: 2.3909850120544434\n",
      "Epoch:  18 iteration 100 loss: 2.396982431411743\n",
      "Epoch:  18 iteration 200 loss: 2.4610283374786377\n",
      "Epoch 18 Training loss 2.3991173754938253\n",
      "Epoch:  19 iteration 0 loss: 2.335995674133301\n",
      "Epoch:  19 iteration 100 loss: 2.2897708415985107\n",
      "Epoch:  19 iteration 200 loss: 2.4485559463500977\n",
      "Epoch 19 Training loss 2.346834864529441\n",
      "Epoch:  20 iteration 0 loss: 2.3194234371185303\n",
      "Epoch:  20 iteration 100 loss: 2.2613048553466797\n",
      "Epoch:  20 iteration 200 loss: 2.3932044506073\n",
      "Epoch 20 Training loss 2.297240606354494\n",
      "Evaluation loss 3.001688482348504\n",
      "BLEU score 0.3572591702415187\n",
      "Epoch:  21 iteration 0 loss: 2.2913753986358643\n",
      "Epoch:  21 iteration 100 loss: 2.207340717315674\n",
      "Epoch:  21 iteration 200 loss: 2.284970283508301\n",
      "Epoch 21 Training loss 2.2551447209649873\n",
      "Epoch:  22 iteration 0 loss: 2.2571303844451904\n",
      "Epoch:  22 iteration 100 loss: 2.153500556945801\n",
      "Epoch:  22 iteration 200 loss: 2.2846293449401855\n",
      "Epoch 22 Training loss 2.21319079503195\n",
      "Epoch:  23 iteration 0 loss: 2.1945714950561523\n",
      "Epoch:  23 iteration 100 loss: 2.0816218852996826\n",
      "Epoch:  23 iteration 200 loss: 2.2070717811584473\n",
      "Epoch 23 Training loss 2.164560999880694\n",
      "Epoch:  24 iteration 0 loss: 2.1757004261016846\n",
      "Epoch:  24 iteration 100 loss: 2.0590298175811768\n",
      "Epoch:  24 iteration 200 loss: 2.1966440677642822\n",
      "Epoch 24 Training loss 2.1315901440311165\n",
      "Epoch:  25 iteration 0 loss: 2.1442880630493164\n",
      "Epoch:  25 iteration 100 loss: 2.028587818145752\n",
      "Epoch:  25 iteration 200 loss: 2.134828805923462\n",
      "Epoch 25 Training loss 2.0922781730579394\n",
      "Evaluation loss 2.9644983029320517\n",
      "BLEU score 0.38979724896761056\n",
      "Epoch:  26 iteration 0 loss: 2.0579395294189453\n",
      "Epoch:  26 iteration 100 loss: 2.0057170391082764\n",
      "Epoch:  26 iteration 200 loss: 2.0974717140197754\n",
      "Epoch 26 Training loss 2.0569813106508663\n",
      "Epoch:  27 iteration 0 loss: 2.066735029220581\n",
      "Epoch:  27 iteration 100 loss: 1.9985766410827637\n",
      "Epoch:  27 iteration 200 loss: 2.0752317905426025\n",
      "Epoch 27 Training loss 2.0209417256793114\n",
      "Epoch:  28 iteration 0 loss: 2.0333399772644043\n",
      "Epoch:  28 iteration 100 loss: 1.913191795349121\n",
      "Epoch:  28 iteration 200 loss: 2.026580333709717\n",
      "Epoch 28 Training loss 1.9892902466237758\n",
      "Epoch:  29 iteration 0 loss: 1.9775162935256958\n",
      "Epoch:  29 iteration 100 loss: 1.9017894268035889\n",
      "Epoch:  29 iteration 200 loss: 2.0157158374786377\n",
      "Epoch 29 Training loss 1.95737764432507\n",
      "Epoch:  30 iteration 0 loss: 1.992505431175232\n",
      "Epoch:  30 iteration 100 loss: 1.8522207736968994\n",
      "Epoch:  30 iteration 200 loss: 1.9482381343841553\n",
      "Epoch 30 Training loss 1.9276253663379914\n",
      "Evaluation loss 2.951551175328009\n",
      "BLEU score 0.36497857370042746\n",
      "Epoch:  31 iteration 0 loss: 1.9226590394973755\n",
      "Epoch:  31 iteration 100 loss: 1.8319414854049683\n",
      "Epoch:  31 iteration 200 loss: 1.9098060131072998\n",
      "Epoch 31 Training loss 1.8967908271973677\n",
      "Epoch:  32 iteration 0 loss: 1.8924628496170044\n",
      "Epoch:  32 iteration 100 loss: 1.768243670463562\n",
      "Epoch:  32 iteration 200 loss: 1.8904156684875488\n",
      "Epoch 32 Training loss 1.8714103582202706\n",
      "Epoch:  33 iteration 0 loss: 1.8506996631622314\n",
      "Epoch:  33 iteration 100 loss: 1.739197015762329\n",
      "Epoch:  33 iteration 200 loss: 1.8091425895690918\n",
      "Epoch 33 Training loss 1.8435889441481506\n",
      "Epoch:  34 iteration 0 loss: 1.8647923469543457\n",
      "Epoch:  34 iteration 100 loss: 1.7391107082366943\n",
      "Epoch:  34 iteration 200 loss: 1.8702503442764282\n",
      "Epoch 34 Training loss 1.8203463934109394\n",
      "Epoch:  35 iteration 0 loss: 1.835077166557312\n",
      "Epoch:  35 iteration 100 loss: 1.6575111150741577\n",
      "Epoch:  35 iteration 200 loss: 1.8723196983337402\n",
      "Epoch 35 Training loss 1.793205685426642\n",
      "Evaluation loss 2.9510709657144845\n",
      "BLEU score 0.41904901485324153\n",
      "Epoch:  36 iteration 0 loss: 1.809106707572937\n",
      "Epoch:  36 iteration 100 loss: 1.6594865322113037\n",
      "Epoch:  36 iteration 200 loss: 1.7833620309829712\n",
      "Epoch 36 Training loss 1.7692316092121867\n",
      "Epoch:  37 iteration 0 loss: 1.7329955101013184\n",
      "Epoch:  37 iteration 100 loss: 1.7162508964538574\n",
      "Epoch:  37 iteration 200 loss: 1.779421329498291\n",
      "Epoch 37 Training loss 1.7460825077326545\n",
      "Epoch:  38 iteration 0 loss: 1.8055042028427124\n",
      "Epoch:  38 iteration 100 loss: 1.6838937997817993\n",
      "Epoch:  38 iteration 200 loss: 1.8020461797714233\n",
      "Epoch 38 Training loss 1.7215548847157554\n",
      "Epoch:  39 iteration 0 loss: 1.736659288406372\n",
      "Epoch:  39 iteration 100 loss: 1.6360045671463013\n",
      "Epoch:  39 iteration 200 loss: 1.7109832763671875\n",
      "Epoch 39 Training loss 1.7026433088635136\n",
      "Epoch:  40 iteration 0 loss: 1.7209101915359497\n",
      "Epoch:  40 iteration 100 loss: 1.605137825012207\n",
      "Epoch:  40 iteration 200 loss: 1.7478036880493164\n",
      "Epoch 40 Training loss 1.6798954753199518\n",
      "Evaluation loss 2.9705529318088013\n",
      "BLEU score 0.38784422302006375\n",
      "Epoch:  41 iteration 0 loss: 1.6946754455566406\n",
      "Epoch:  41 iteration 100 loss: 1.5825750827789307\n",
      "Epoch:  41 iteration 200 loss: 1.6562000513076782\n",
      "Epoch 41 Training loss 1.660973764906388\n",
      "Epoch:  42 iteration 0 loss: 1.6302590370178223\n",
      "Epoch:  42 iteration 100 loss: 1.5902011394500732\n",
      "Epoch:  42 iteration 200 loss: 1.6528451442718506\n",
      "Epoch 42 Training loss 1.638149978311482\n",
      "Epoch:  43 iteration 0 loss: 1.6371062994003296\n",
      "Epoch:  43 iteration 100 loss: 1.5693305730819702\n",
      "Epoch:  43 iteration 200 loss: 1.6367509365081787\n",
      "Epoch 43 Training loss 1.617709642965598\n",
      "Epoch:  44 iteration 0 loss: 1.6641000509262085\n",
      "Epoch:  44 iteration 100 loss: 1.517520546913147\n",
      "Epoch:  44 iteration 200 loss: 1.5651791095733643\n",
      "Epoch 44 Training loss 1.598292138136111\n",
      "Epoch:  45 iteration 0 loss: 1.6472153663635254\n",
      "Epoch:  45 iteration 100 loss: 1.470855712890625\n",
      "Epoch:  45 iteration 200 loss: 1.606005311012268\n",
      "Epoch 45 Training loss 1.5835742534810164\n",
      "Evaluation loss 2.990612892317734\n",
      "BLEU score 0.4049149820310493\n",
      "Epoch:  46 iteration 0 loss: 1.624116063117981\n",
      "Epoch:  46 iteration 100 loss: 1.4933198690414429\n",
      "Epoch:  46 iteration 200 loss: 1.5536683797836304\n",
      "Epoch 46 Training loss 1.563064688238062\n",
      "Epoch:  47 iteration 0 loss: 1.570029854774475\n",
      "Epoch:  47 iteration 100 loss: 1.4483931064605713\n",
      "Epoch:  47 iteration 200 loss: 1.5711919069290161\n",
      "Epoch 47 Training loss 1.5468485395113378\n",
      "Epoch:  48 iteration 0 loss: 1.5565601587295532\n",
      "Epoch:  48 iteration 100 loss: 1.3767118453979492\n",
      "Epoch:  48 iteration 200 loss: 1.514440655708313\n",
      "Epoch 48 Training loss 1.5312507541341431\n",
      "Epoch:  49 iteration 0 loss: 1.5400339365005493\n",
      "Epoch:  49 iteration 100 loss: 1.443447232246399\n",
      "Epoch:  49 iteration 200 loss: 1.5362850427627563\n",
      "Epoch 49 Training loss 1.5132347583799206\n",
      "Epoch:  50 iteration 0 loss: 1.514540433883667\n",
      "Epoch:  50 iteration 100 loss: 1.4104666709899902\n",
      "Epoch:  50 iteration 200 loss: 1.5062193870544434\n",
      "Epoch 50 Training loss 1.499088496660934\n",
      "Evaluation loss 3.0038810522725568\n",
      "BLEU score 0.4079312688685132\n",
      "Epoch:  51 iteration 0 loss: 1.4617595672607422\n",
      "Epoch:  51 iteration 100 loss: 1.384334683418274\n",
      "Epoch:  51 iteration 200 loss: 1.5581179857254028\n",
      "Epoch 51 Training loss 1.4807101048694076\n",
      "Epoch:  52 iteration 0 loss: 1.4906591176986694\n",
      "Epoch:  52 iteration 100 loss: 1.383454442024231\n",
      "Epoch:  52 iteration 200 loss: 1.511527419090271\n",
      "Epoch 52 Training loss 1.4687483144596014\n",
      "Epoch:  53 iteration 0 loss: 1.4417976140975952\n",
      "Epoch:  53 iteration 100 loss: 1.3976625204086304\n",
      "Epoch:  53 iteration 200 loss: 1.4763092994689941\n",
      "Epoch 53 Training loss 1.4512039019711405\n",
      "Epoch:  54 iteration 0 loss: 1.4400237798690796\n",
      "Epoch:  54 iteration 100 loss: 1.330186367034912\n",
      "Epoch:  54 iteration 200 loss: 1.4487210512161255\n",
      "Epoch 54 Training loss 1.4342534642878775\n",
      "Epoch:  55 iteration 0 loss: 1.4472358226776123\n",
      "Epoch:  55 iteration 100 loss: 1.2952885627746582\n",
      "Epoch:  55 iteration 200 loss: 1.390302300453186\n",
      "Epoch 55 Training loss 1.4245911984752149\n",
      "Evaluation loss 3.031008980963306\n",
      "BLEU score 0.42486586851263857\n",
      "Epoch:  56 iteration 0 loss: 1.3916281461715698\n",
      "Epoch:  56 iteration 100 loss: 1.3182661533355713\n",
      "Epoch:  56 iteration 200 loss: 1.4056326150894165\n",
      "Epoch 56 Training loss 1.4118588940003651\n",
      "Epoch:  57 iteration 0 loss: 1.4335581064224243\n",
      "Epoch:  57 iteration 100 loss: 1.312544584274292\n",
      "Epoch:  57 iteration 200 loss: 1.391483187675476\n",
      "Epoch 57 Training loss 1.3940559722770143\n",
      "Epoch:  58 iteration 0 loss: 1.3878443241119385\n",
      "Epoch:  58 iteration 100 loss: 1.2610790729522705\n",
      "Epoch:  58 iteration 200 loss: 1.368589997291565\n",
      "Epoch 58 Training loss 1.3811927021576407\n",
      "Epoch:  59 iteration 0 loss: 1.3739328384399414\n",
      "Epoch:  59 iteration 100 loss: 1.2671087980270386\n",
      "Epoch:  59 iteration 200 loss: 1.4300440549850464\n",
      "Epoch 59 Training loss 1.36830949380497\n",
      "Epoch:  60 iteration 0 loss: 1.3408522605895996\n",
      "Epoch:  60 iteration 100 loss: 1.259588599205017\n",
      "Epoch:  60 iteration 200 loss: 1.391603708267212\n",
      "Epoch 60 Training loss 1.3568542166874544\n",
      "Evaluation loss 3.0536947774100995\n",
      "BLEU score 0.3755966358499135\n",
      "Epoch:  61 iteration 0 loss: 1.3381325006484985\n",
      "Epoch:  61 iteration 100 loss: 1.2396615743637085\n",
      "Epoch:  61 iteration 200 loss: 1.3446991443634033\n",
      "Epoch 61 Training loss 1.3422746825952494\n",
      "Epoch:  62 iteration 0 loss: 1.3303415775299072\n",
      "Epoch:  62 iteration 100 loss: 1.1865781545639038\n",
      "Epoch:  62 iteration 200 loss: 1.331619381904602\n",
      "Epoch 62 Training loss 1.3313495355144052\n",
      "Epoch:  63 iteration 0 loss: 1.3560529947280884\n",
      "Epoch:  63 iteration 100 loss: 1.232697606086731\n",
      "Epoch:  63 iteration 200 loss: 1.3292951583862305\n",
      "Epoch 63 Training loss 1.3186142882902947\n",
      "Epoch:  64 iteration 0 loss: 1.3356331586837769\n",
      "Epoch:  64 iteration 100 loss: 1.1969380378723145\n",
      "Epoch:  64 iteration 200 loss: 1.299484133720398\n",
      "Epoch 64 Training loss 1.3073742517910787\n",
      "Epoch:  65 iteration 0 loss: 1.3378626108169556\n",
      "Epoch:  65 iteration 100 loss: 1.1844877004623413\n",
      "Epoch:  65 iteration 200 loss: 1.3045800924301147\n",
      "Epoch 65 Training loss 1.2954091206771434\n",
      "Evaluation loss 3.076703544196738\n",
      "BLEU score 0.39921338554865073\n",
      "Epoch:  66 iteration 0 loss: 1.3555560111999512\n",
      "Epoch:  66 iteration 100 loss: 1.188116192817688\n",
      "Epoch:  66 iteration 200 loss: 1.304133415222168\n",
      "Epoch 66 Training loss 1.282453807299663\n",
      "Epoch:  67 iteration 0 loss: 1.3177943229675293\n",
      "Epoch:  67 iteration 100 loss: 1.2151775360107422\n",
      "Epoch:  67 iteration 200 loss: 1.2528648376464844\n",
      "Epoch 67 Training loss 1.2760704999101196\n",
      "Epoch:  68 iteration 0 loss: 1.2671343088150024\n",
      "Epoch:  68 iteration 100 loss: 1.1414985656738281\n",
      "Epoch:  68 iteration 200 loss: 1.2457331418991089\n",
      "Epoch 68 Training loss 1.260075810876145\n",
      "Epoch:  69 iteration 0 loss: 1.3229150772094727\n",
      "Epoch:  69 iteration 100 loss: 1.2082246541976929\n",
      "Epoch:  69 iteration 200 loss: 1.2633744478225708\n",
      "Epoch 69 Training loss 1.2553532260397715\n",
      "Epoch:  70 iteration 0 loss: 1.2530666589736938\n",
      "Epoch:  70 iteration 100 loss: 1.161337971687317\n",
      "Epoch:  70 iteration 200 loss: 1.1937016248703003\n",
      "Epoch 70 Training loss 1.2442008939679625\n",
      "Evaluation loss 3.1029818144410686\n",
      "BLEU score 0.41803486356253517\n",
      "Epoch:  71 iteration 0 loss: 1.240131139755249\n",
      "Epoch:  71 iteration 100 loss: 1.1418908834457397\n",
      "Epoch:  71 iteration 200 loss: 1.1885889768600464\n",
      "Epoch 71 Training loss 1.234544232558255\n",
      "Epoch:  72 iteration 0 loss: 1.2307707071304321\n",
      "Epoch:  72 iteration 100 loss: 1.1538623571395874\n",
      "Epoch:  72 iteration 200 loss: 1.277408480644226\n",
      "Epoch 72 Training loss 1.221541576216272\n",
      "Epoch:  73 iteration 0 loss: 1.2292858362197876\n",
      "Epoch:  73 iteration 100 loss: 1.1166912317276\n",
      "Epoch:  73 iteration 200 loss: 1.180471658706665\n",
      "Epoch 73 Training loss 1.2094018295637183\n",
      "Epoch:  74 iteration 0 loss: 1.1927882432937622\n",
      "Epoch:  74 iteration 100 loss: 1.090549111366272\n",
      "Epoch:  74 iteration 200 loss: 1.2174043655395508\n",
      "Epoch 74 Training loss 1.2024561085123109\n",
      "Epoch:  75 iteration 0 loss: 1.183396816253662\n",
      "Epoch:  75 iteration 100 loss: 1.0726441144943237\n",
      "Epoch:  75 iteration 200 loss: 1.159580945968628\n",
      "Epoch 75 Training loss 1.1916606319586327\n",
      "Evaluation loss 3.1333684763803493\n",
      "BLEU score 0.41022006141883145\n",
      "Epoch:  76 iteration 0 loss: 1.1997677087783813\n",
      "Epoch:  76 iteration 100 loss: 1.0076115131378174\n",
      "Epoch:  76 iteration 200 loss: 1.148439645767212\n",
      "Epoch 76 Training loss 1.184177935782406\n",
      "Epoch:  77 iteration 0 loss: 1.1926758289337158\n",
      "Epoch:  77 iteration 100 loss: 1.0806471109390259\n",
      "Epoch:  77 iteration 200 loss: 1.1381067037582397\n",
      "Epoch 77 Training loss 1.1772042272606416\n",
      "Epoch:  78 iteration 0 loss: 1.1965185403823853\n",
      "Epoch:  78 iteration 100 loss: 1.0900607109069824\n",
      "Epoch:  78 iteration 200 loss: 1.1477382183074951\n",
      "Epoch 78 Training loss 1.1686177953511239\n",
      "Epoch:  79 iteration 0 loss: 1.1336824893951416\n",
      "Epoch:  79 iteration 100 loss: 1.0780515670776367\n",
      "Epoch:  79 iteration 200 loss: 1.108048915863037\n",
      "Epoch 79 Training loss 1.1564792570193503\n",
      "Epoch:  80 iteration 0 loss: 1.1323615312576294\n",
      "Epoch:  80 iteration 100 loss: 1.0198231935501099\n",
      "Epoch:  80 iteration 200 loss: 1.1794867515563965\n",
      "Epoch 80 Training loss 1.1488226657362794\n",
      "Evaluation loss 3.157514263354198\n",
      "BLEU score 0.44134184360062505\n",
      "Epoch:  81 iteration 0 loss: 1.1104991436004639\n",
      "Epoch:  81 iteration 100 loss: 1.0163308382034302\n",
      "Epoch:  81 iteration 200 loss: 1.112069010734558\n",
      "Epoch 81 Training loss 1.1401965151535765\n",
      "Epoch:  82 iteration 0 loss: 1.1395416259765625\n",
      "Epoch:  82 iteration 100 loss: 0.9931308031082153\n",
      "Epoch:  82 iteration 200 loss: 1.1592135429382324\n",
      "Epoch 82 Training loss 1.13214160602898\n",
      "Epoch:  83 iteration 0 loss: 1.127294898033142\n",
      "Epoch:  83 iteration 100 loss: 0.972713053226471\n",
      "Epoch:  83 iteration 200 loss: 1.1388293504714966\n",
      "Epoch 83 Training loss 1.1222311236916611\n",
      "Epoch:  84 iteration 0 loss: 1.146419644355774\n",
      "Epoch:  84 iteration 100 loss: 0.9752309918403625\n",
      "Epoch:  84 iteration 200 loss: 1.158468246459961\n",
      "Epoch 84 Training loss 1.115461383923705\n",
      "Epoch:  85 iteration 0 loss: 1.133973240852356\n",
      "Epoch:  85 iteration 100 loss: 1.020156741142273\n",
      "Epoch:  85 iteration 200 loss: 1.1089274883270264\n",
      "Epoch 85 Training loss 1.110385948420357\n",
      "Evaluation loss 3.1834485390793157\n",
      "BLEU score 0.46105408163267777\n",
      "Epoch:  86 iteration 0 loss: 1.124455213546753\n",
      "Epoch:  86 iteration 100 loss: 1.0329277515411377\n",
      "Epoch:  86 iteration 200 loss: 1.1320269107818604\n",
      "Epoch 86 Training loss 1.1005568705867406\n",
      "Epoch:  87 iteration 0 loss: 1.1317540407180786\n",
      "Epoch:  87 iteration 100 loss: 0.9646696448326111\n",
      "Epoch:  87 iteration 200 loss: 1.0715659856796265\n",
      "Epoch 87 Training loss 1.0873963134280222\n",
      "Epoch:  88 iteration 0 loss: 1.04542076587677\n",
      "Epoch:  88 iteration 100 loss: 0.9461202025413513\n",
      "Epoch:  88 iteration 200 loss: 1.021449327468872\n",
      "Epoch 88 Training loss 1.0841759615652289\n",
      "Epoch:  89 iteration 0 loss: 1.0782917737960815\n",
      "Epoch:  89 iteration 100 loss: 0.9736188650131226\n",
      "Epoch:  89 iteration 200 loss: 1.0223315954208374\n",
      "Epoch 89 Training loss 1.0791286903237942\n",
      "Epoch:  90 iteration 0 loss: 1.0354909896850586\n",
      "Epoch:  90 iteration 100 loss: 0.9433243870735168\n",
      "Epoch:  90 iteration 200 loss: 1.0861729383468628\n",
      "Epoch 90 Training loss 1.069678295269212\n",
      "Evaluation loss 3.2086267403777975\n",
      "BLEU score 0.4234248398345704\n",
      "Epoch:  91 iteration 0 loss: 1.0603315830230713\n",
      "Epoch:  91 iteration 100 loss: 0.9141501188278198\n",
      "Epoch:  91 iteration 200 loss: 1.0326496362686157\n",
      "Epoch 91 Training loss 1.0633462676801135\n",
      "Epoch:  92 iteration 0 loss: 1.0205440521240234\n",
      "Epoch:  92 iteration 100 loss: 0.9881988763809204\n",
      "Epoch:  92 iteration 200 loss: 1.023561716079712\n",
      "Epoch 92 Training loss 1.0547701865005947\n",
      "Epoch:  93 iteration 0 loss: 1.0376472473144531\n",
      "Epoch:  93 iteration 100 loss: 0.9240049123764038\n",
      "Epoch:  93 iteration 200 loss: 1.0065033435821533\n",
      "Epoch 93 Training loss 1.0491006148479183\n",
      "Epoch:  94 iteration 0 loss: 1.0423102378845215\n",
      "Epoch:  94 iteration 100 loss: 0.9420216679573059\n",
      "Epoch:  94 iteration 200 loss: 0.9502296447753906\n",
      "Epoch 94 Training loss 1.041026654071862\n",
      "Epoch:  95 iteration 0 loss: 1.052868127822876\n",
      "Epoch:  95 iteration 100 loss: 0.9507314562797546\n",
      "Epoch:  95 iteration 200 loss: 0.9711620807647705\n",
      "Epoch 95 Training loss 1.0388182903895016\n",
      "Evaluation loss 3.236868681370047\n",
      "BLEU score 0.42060646549311925\n",
      "Epoch:  96 iteration 0 loss: 1.021747350692749\n",
      "Epoch:  96 iteration 100 loss: 0.9263226985931396\n",
      "Epoch:  96 iteration 200 loss: 0.9882539510726929\n",
      "Epoch 96 Training loss 1.0270984478038565\n",
      "Epoch:  97 iteration 0 loss: 1.0199475288391113\n",
      "Epoch:  97 iteration 100 loss: 0.875187337398529\n",
      "Epoch:  97 iteration 200 loss: 0.9632239937782288\n",
      "Epoch 97 Training loss 1.0184045746353763\n",
      "Epoch:  98 iteration 0 loss: 0.9960498809814453\n",
      "Epoch:  98 iteration 100 loss: 0.8637528419494629\n",
      "Epoch:  98 iteration 200 loss: 1.002433180809021\n",
      "Epoch 98 Training loss 1.0136708993603334\n",
      "Epoch:  99 iteration 0 loss: 0.9557904601097107\n",
      "Epoch:  99 iteration 100 loss: 0.8970420956611633\n",
      "Epoch:  99 iteration 200 loss: 0.9285996556282043\n",
      "Epoch 99 Training loss 1.00506894513184\n",
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有任何一个小人。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你是無法的。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "每個人都喜歡自己的手。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "什么时候？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我今晚有空。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "這本書是你的書。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們在午餐。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "这辆卡脚是困难。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它真的很好。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "許多那麼尊敬他產禮物。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "还没有回家。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "很少人。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我摑了他的臉。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜欢红鞋音乐。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆没有孩子。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請把門打門。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆还得到了。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請德語說。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "在下午有人下下。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我犯了一個错了。\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "\n",
    "# GRU\n",
    "model_gru_luong = Seq2Seq(rnn_type='GRU', attn_type='luong')\n",
    "model_gru_luong = model_gru_luong.to(device)\n",
    "loss_fn = MaskedCrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model_gru_luong.parameters())\n",
    "train(model_gru_luong, train_batches, num_epochs=100)\n",
    "\n",
    "for i in range(100, 120):\n",
    "    translate_dev(model_gru_luong, i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3adca69a77ec25e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T03:02:55.725561200Z",
     "start_time": "2024-06-23T02:56:09.637382300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 iteration 0 loss: 8.078397750854492\n",
      "Epoch:  0 iteration 100 loss: 5.240570068359375\n",
      "Epoch:  0 iteration 200 loss: 5.232659339904785\n",
      "Epoch 0 Training loss 5.560061845621507\n",
      "Evaluation loss 5.114669716060663\n",
      "BLEU score 0.3596936498613376\n",
      "Epoch:  1 iteration 0 loss: 5.200769901275635\n",
      "Epoch:  1 iteration 100 loss: 4.840377330780029\n",
      "Epoch:  1 iteration 200 loss: 4.861570835113525\n",
      "Epoch 1 Training loss 4.952638143717205\n",
      "Epoch:  2 iteration 0 loss: 4.826529026031494\n",
      "Epoch:  2 iteration 100 loss: 4.460938453674316\n",
      "Epoch:  2 iteration 200 loss: 4.525976657867432\n",
      "Epoch 2 Training loss 4.561297351079819\n",
      "Epoch:  3 iteration 0 loss: 4.4459333419799805\n",
      "Epoch:  3 iteration 100 loss: 4.137321472167969\n",
      "Epoch:  3 iteration 200 loss: 4.25966739654541\n",
      "Epoch 3 Training loss 4.228825152872819\n",
      "Epoch:  4 iteration 0 loss: 4.164528846740723\n",
      "Epoch:  4 iteration 100 loss: 3.848923444747925\n",
      "Epoch:  4 iteration 200 loss: 4.011579513549805\n",
      "Epoch 4 Training loss 3.953392958859109\n",
      "Epoch:  5 iteration 0 loss: 3.873133659362793\n",
      "Epoch:  5 iteration 100 loss: 3.6198949813842773\n",
      "Epoch:  5 iteration 200 loss: 3.802839994430542\n",
      "Epoch 5 Training loss 3.725104776772816\n",
      "Evaluation loss 3.7122424212238516\n",
      "BLEU score 0.3801722293158932\n",
      "Epoch:  6 iteration 0 loss: 3.6530048847198486\n",
      "Epoch:  6 iteration 100 loss: 3.455975294113159\n",
      "Epoch:  6 iteration 200 loss: 3.6120524406433105\n",
      "Epoch 6 Training loss 3.530379381358688\n",
      "Epoch:  7 iteration 0 loss: 3.4687981605529785\n",
      "Epoch:  7 iteration 100 loss: 3.277951717376709\n",
      "Epoch:  7 iteration 200 loss: 3.424532651901245\n",
      "Epoch 7 Training loss 3.352582504170969\n",
      "Epoch:  8 iteration 0 loss: 3.257050037384033\n",
      "Epoch:  8 iteration 100 loss: 3.1295673847198486\n",
      "Epoch:  8 iteration 200 loss: 3.2665200233459473\n",
      "Epoch 8 Training loss 3.195792039287351\n",
      "Epoch:  9 iteration 0 loss: 3.1457748413085938\n",
      "Epoch:  9 iteration 100 loss: 2.9999077320098877\n",
      "Epoch:  9 iteration 200 loss: 3.1591429710388184\n",
      "Epoch 9 Training loss 3.053241763034895\n",
      "Epoch:  10 iteration 0 loss: 2.969041585922241\n",
      "Epoch:  10 iteration 100 loss: 2.8429534435272217\n",
      "Epoch:  10 iteration 200 loss: 2.9917685985565186\n",
      "Epoch 10 Training loss 2.922428085775141\n",
      "Evaluation loss 3.1762814365180194\n",
      "BLEU score 0.3637051392342203\n",
      "Epoch:  11 iteration 0 loss: 2.79315185546875\n",
      "Epoch:  11 iteration 100 loss: 2.734274387359619\n",
      "Epoch:  11 iteration 200 loss: 2.864295244216919\n",
      "Epoch 11 Training loss 2.801951797694464\n",
      "Epoch:  12 iteration 0 loss: 2.703005075454712\n",
      "Epoch:  12 iteration 100 loss: 2.626387596130371\n",
      "Epoch:  12 iteration 200 loss: 2.7642693519592285\n",
      "Epoch 12 Training loss 2.698549584328923\n",
      "Epoch:  13 iteration 0 loss: 2.6269590854644775\n",
      "Epoch:  13 iteration 100 loss: 2.550546169281006\n",
      "Epoch:  13 iteration 200 loss: 2.64635968208313\n",
      "Epoch 13 Training loss 2.5955204138281287\n",
      "Epoch:  14 iteration 0 loss: 2.4992573261260986\n",
      "Epoch:  14 iteration 100 loss: 2.4072351455688477\n",
      "Epoch:  14 iteration 200 loss: 2.5735747814178467\n",
      "Epoch 14 Training loss 2.505811969304151\n",
      "Epoch:  15 iteration 0 loss: 2.358551263809204\n",
      "Epoch:  15 iteration 100 loss: 2.329225540161133\n",
      "Epoch:  15 iteration 200 loss: 2.5049824714660645\n",
      "Epoch 15 Training loss 2.4235180816343913\n",
      "Evaluation loss 2.9169875692032576\n",
      "BLEU score 0.36836215226806156\n",
      "Epoch:  16 iteration 0 loss: 2.3583173751831055\n",
      "Epoch:  16 iteration 100 loss: 2.2630956172943115\n",
      "Epoch:  16 iteration 200 loss: 2.4441840648651123\n",
      "Epoch 16 Training loss 2.3454842650660312\n",
      "Epoch:  17 iteration 0 loss: 2.2449042797088623\n",
      "Epoch:  17 iteration 100 loss: 2.1408355236053467\n",
      "Epoch:  17 iteration 200 loss: 2.3547050952911377\n",
      "Epoch 17 Training loss 2.271964794193076\n",
      "Epoch:  18 iteration 0 loss: 2.195668935775757\n",
      "Epoch:  18 iteration 100 loss: 2.096203327178955\n",
      "Epoch:  18 iteration 200 loss: 2.248257875442505\n",
      "Epoch 18 Training loss 2.2022164696356006\n",
      "Epoch:  19 iteration 0 loss: 2.116123914718628\n",
      "Epoch:  19 iteration 100 loss: 2.0809552669525146\n",
      "Epoch:  19 iteration 200 loss: 2.159245252609253\n",
      "Epoch 19 Training loss 2.1431775038414687\n",
      "Epoch:  20 iteration 0 loss: 2.0258371829986572\n",
      "Epoch:  20 iteration 100 loss: 1.9637768268585205\n",
      "Epoch:  20 iteration 200 loss: 2.131060838699341\n",
      "Epoch 20 Training loss 2.0821911938854094\n",
      "Evaluation loss 2.78929379016415\n",
      "BLEU score 0.3670461087392692\n",
      "Epoch:  21 iteration 0 loss: 1.9517589807510376\n",
      "Epoch:  21 iteration 100 loss: 1.9164023399353027\n",
      "Epoch:  21 iteration 200 loss: 2.091978073120117\n",
      "Epoch 21 Training loss 2.0302761245291747\n",
      "Epoch:  22 iteration 0 loss: 1.9372729063034058\n",
      "Epoch:  22 iteration 100 loss: 1.8678622245788574\n",
      "Epoch:  22 iteration 200 loss: 2.011445999145508\n",
      "Epoch 22 Training loss 1.9739086120671496\n",
      "Epoch:  23 iteration 0 loss: 1.8663398027420044\n",
      "Epoch:  23 iteration 100 loss: 1.8346161842346191\n",
      "Epoch:  23 iteration 200 loss: 1.9587721824645996\n",
      "Epoch 23 Training loss 1.9280017543705885\n",
      "Epoch:  24 iteration 0 loss: 1.8127604722976685\n",
      "Epoch:  24 iteration 100 loss: 1.7822167873382568\n",
      "Epoch:  24 iteration 200 loss: 1.9006781578063965\n",
      "Epoch 24 Training loss 1.8780787041372755\n",
      "Epoch:  25 iteration 0 loss: 1.7964731454849243\n",
      "Epoch:  25 iteration 100 loss: 1.7056511640548706\n",
      "Epoch:  25 iteration 200 loss: 1.8814750909805298\n",
      "Epoch 25 Training loss 1.8370415041739714\n",
      "Evaluation loss 2.7389276829100746\n",
      "BLEU score 0.386454094794998\n",
      "Epoch:  26 iteration 0 loss: 1.7372204065322876\n",
      "Epoch:  26 iteration 100 loss: 1.6774252653121948\n",
      "Epoch:  26 iteration 200 loss: 1.8008801937103271\n",
      "Epoch 26 Training loss 1.7899213188981593\n",
      "Epoch:  27 iteration 0 loss: 1.7207962274551392\n",
      "Epoch:  27 iteration 100 loss: 1.6179534196853638\n",
      "Epoch:  27 iteration 200 loss: 1.811959147453308\n",
      "Epoch 27 Training loss 1.7509397086148353\n",
      "Epoch:  28 iteration 0 loss: 1.7032712697982788\n",
      "Epoch:  28 iteration 100 loss: 1.5993303060531616\n",
      "Epoch:  28 iteration 200 loss: 1.702246069908142\n",
      "Epoch 28 Training loss 1.7141371013791769\n",
      "Epoch:  29 iteration 0 loss: 1.6691341400146484\n",
      "Epoch:  29 iteration 100 loss: 1.5561413764953613\n",
      "Epoch:  29 iteration 200 loss: 1.6909762620925903\n",
      "Epoch 29 Training loss 1.6754046840992294\n",
      "Epoch:  30 iteration 0 loss: 1.5783628225326538\n",
      "Epoch:  30 iteration 100 loss: 1.535849928855896\n",
      "Epoch:  30 iteration 200 loss: 1.6843066215515137\n",
      "Epoch 30 Training loss 1.64257876425828\n",
      "Evaluation loss 2.699742903322695\n",
      "BLEU score 0.39506189859401766\n",
      "Epoch:  31 iteration 0 loss: 1.6036834716796875\n",
      "Epoch:  31 iteration 100 loss: 1.4883712530136108\n",
      "Epoch:  31 iteration 200 loss: 1.6115131378173828\n",
      "Epoch 31 Training loss 1.6121568716479329\n",
      "Epoch:  32 iteration 0 loss: 1.546897053718567\n",
      "Epoch:  32 iteration 100 loss: 1.4202479124069214\n",
      "Epoch:  32 iteration 200 loss: 1.6249735355377197\n",
      "Epoch 32 Training loss 1.581543690277861\n",
      "Epoch:  33 iteration 0 loss: 1.5406590700149536\n",
      "Epoch:  33 iteration 100 loss: 1.448184847831726\n",
      "Epoch:  33 iteration 200 loss: 1.5889885425567627\n",
      "Epoch 33 Training loss 1.5530140332276894\n",
      "Epoch:  34 iteration 0 loss: 1.4808671474456787\n",
      "Epoch:  34 iteration 100 loss: 1.3753771781921387\n",
      "Epoch:  34 iteration 200 loss: 1.5433158874511719\n",
      "Epoch 34 Training loss 1.5215008369877205\n",
      "Epoch:  35 iteration 0 loss: 1.4827179908752441\n",
      "Epoch:  35 iteration 100 loss: 1.3484165668487549\n",
      "Epoch:  35 iteration 200 loss: 1.525965929031372\n",
      "Epoch 35 Training loss 1.5007172252575403\n",
      "Evaluation loss 2.697813234668483\n",
      "BLEU score 0.39828721604166947\n",
      "Epoch:  36 iteration 0 loss: 1.4298179149627686\n",
      "Epoch:  36 iteration 100 loss: 1.2934415340423584\n",
      "Epoch:  36 iteration 200 loss: 1.4564547538757324\n",
      "Epoch 36 Training loss 1.4686026885724013\n",
      "Epoch:  37 iteration 0 loss: 1.4547219276428223\n",
      "Epoch:  37 iteration 100 loss: 1.30203115940094\n",
      "Epoch:  37 iteration 200 loss: 1.4574649333953857\n",
      "Epoch 37 Training loss 1.4408855390251725\n",
      "Epoch:  38 iteration 0 loss: 1.38760507106781\n",
      "Epoch:  38 iteration 100 loss: 1.2789548635482788\n",
      "Epoch:  38 iteration 200 loss: 1.4377245903015137\n",
      "Epoch 38 Training loss 1.4140131178326114\n",
      "Epoch:  39 iteration 0 loss: 1.363877773284912\n",
      "Epoch:  39 iteration 100 loss: 1.332053542137146\n",
      "Epoch:  39 iteration 200 loss: 1.4332835674285889\n",
      "Epoch 39 Training loss 1.3934205322881126\n",
      "Epoch:  40 iteration 0 loss: 1.3207272291183472\n",
      "Epoch:  40 iteration 100 loss: 1.2633014917373657\n",
      "Epoch:  40 iteration 200 loss: 1.4257862567901611\n",
      "Epoch 40 Training loss 1.369463916532406\n",
      "Evaluation loss 2.6947774571432537\n",
      "BLEU score 0.39635344035339284\n",
      "Epoch:  41 iteration 0 loss: 1.3301969766616821\n",
      "Epoch:  41 iteration 100 loss: 1.2601343393325806\n",
      "Epoch:  41 iteration 200 loss: 1.4211008548736572\n",
      "Epoch 41 Training loss 1.3489298882984708\n",
      "Epoch:  42 iteration 0 loss: 1.310716152191162\n",
      "Epoch:  42 iteration 100 loss: 1.1903977394104004\n",
      "Epoch:  42 iteration 200 loss: 1.359243631362915\n",
      "Epoch 42 Training loss 1.327844369818902\n",
      "Epoch:  43 iteration 0 loss: 1.2998446226119995\n",
      "Epoch:  43 iteration 100 loss: 1.1597580909729004\n",
      "Epoch:  43 iteration 200 loss: 1.3036614656448364\n",
      "Epoch 43 Training loss 1.3030415853466886\n",
      "Epoch:  44 iteration 0 loss: 1.2993789911270142\n",
      "Epoch:  44 iteration 100 loss: 1.156793236732483\n",
      "Epoch:  44 iteration 200 loss: 1.2619916200637817\n",
      "Epoch 44 Training loss 1.2879215855774901\n",
      "Epoch:  45 iteration 0 loss: 1.264026165008545\n",
      "Epoch:  45 iteration 100 loss: 1.086113691329956\n",
      "Epoch:  45 iteration 200 loss: 1.2414478063583374\n",
      "Epoch 45 Training loss 1.2680903401986514\n",
      "Evaluation loss 2.712029114072772\n",
      "BLEU score 0.36791521832769485\n",
      "Epoch:  46 iteration 0 loss: 1.232385277748108\n",
      "Epoch:  46 iteration 100 loss: 1.1337766647338867\n",
      "Epoch:  46 iteration 200 loss: 1.2537767887115479\n",
      "Epoch 46 Training loss 1.2492919187934683\n",
      "Epoch:  47 iteration 0 loss: 1.293363332748413\n",
      "Epoch:  47 iteration 100 loss: 1.0853136777877808\n",
      "Epoch:  47 iteration 200 loss: 1.2302666902542114\n",
      "Epoch 47 Training loss 1.2273319739573276\n",
      "Epoch:  48 iteration 0 loss: 1.167497158050537\n",
      "Epoch:  48 iteration 100 loss: 1.0215001106262207\n",
      "Epoch:  48 iteration 200 loss: 1.2023757696151733\n",
      "Epoch 48 Training loss 1.212491156635247\n",
      "Epoch:  49 iteration 0 loss: 1.2161507606506348\n",
      "Epoch:  49 iteration 100 loss: 1.1025490760803223\n",
      "Epoch:  49 iteration 200 loss: 1.230380892753601\n",
      "Epoch 49 Training loss 1.2006624904873173\n",
      "Epoch:  50 iteration 0 loss: 1.1499897241592407\n",
      "Epoch:  50 iteration 100 loss: 1.0474451780319214\n",
      "Epoch:  50 iteration 200 loss: 1.2515535354614258\n",
      "Epoch 50 Training loss 1.1782751876568163\n",
      "Evaluation loss 2.7257543696334046\n",
      "BLEU score 0.3619130208698516\n",
      "Epoch:  51 iteration 0 loss: 1.153259515762329\n",
      "Epoch:  51 iteration 100 loss: 1.0367385149002075\n",
      "Epoch:  51 iteration 200 loss: 1.1799087524414062\n",
      "Epoch 51 Training loss 1.1648032881152723\n",
      "Epoch:  52 iteration 0 loss: 1.1331907510757446\n",
      "Epoch:  52 iteration 100 loss: 1.0307377576828003\n",
      "Epoch:  52 iteration 200 loss: 1.137712001800537\n",
      "Epoch 52 Training loss 1.1465748446733572\n",
      "Epoch:  53 iteration 0 loss: 1.1842283010482788\n",
      "Epoch:  53 iteration 100 loss: 1.055609107017517\n",
      "Epoch:  53 iteration 200 loss: 1.1752887964248657\n",
      "Epoch 53 Training loss 1.132681973503356\n",
      "Epoch:  54 iteration 0 loss: 1.1031193733215332\n",
      "Epoch:  54 iteration 100 loss: 0.9922319650650024\n",
      "Epoch:  54 iteration 200 loss: 1.1333645582199097\n",
      "Epoch 54 Training loss 1.1171730132390918\n",
      "Epoch:  55 iteration 0 loss: 1.1410948038101196\n",
      "Epoch:  55 iteration 100 loss: 1.0084419250488281\n",
      "Epoch:  55 iteration 200 loss: 1.0719717741012573\n",
      "Epoch 55 Training loss 1.1033224479805845\n",
      "Evaluation loss 2.759936513369234\n",
      "BLEU score 0.3627413765778461\n",
      "Epoch:  56 iteration 0 loss: 1.0867853164672852\n",
      "Epoch:  56 iteration 100 loss: 0.961143434047699\n",
      "Epoch:  56 iteration 200 loss: 1.1503033638000488\n",
      "Epoch 56 Training loss 1.0899701026918396\n",
      "Epoch:  57 iteration 0 loss: 1.0833463668823242\n",
      "Epoch:  57 iteration 100 loss: 0.9182757139205933\n",
      "Epoch:  57 iteration 200 loss: 1.0374740362167358\n",
      "Epoch 57 Training loss 1.0705370981734932\n",
      "Epoch:  58 iteration 0 loss: 1.029801607131958\n",
      "Epoch:  58 iteration 100 loss: 0.9144269824028015\n",
      "Epoch:  58 iteration 200 loss: 1.088073492050171\n",
      "Epoch 58 Training loss 1.0623531248783187\n",
      "Epoch:  59 iteration 0 loss: 1.0852315425872803\n",
      "Epoch:  59 iteration 100 loss: 0.9083049893379211\n",
      "Epoch:  59 iteration 200 loss: 1.0637203454971313\n",
      "Epoch 59 Training loss 1.0476128309201336\n",
      "Epoch:  60 iteration 0 loss: 1.031556487083435\n",
      "Epoch:  60 iteration 100 loss: 0.8865063190460205\n",
      "Epoch:  60 iteration 200 loss: 1.0310180187225342\n",
      "Epoch 60 Training loss 1.0370942816387625\n",
      "Evaluation loss 2.78122068203494\n",
      "BLEU score 0.36154413874946034\n",
      "Epoch:  61 iteration 0 loss: 1.0157805681228638\n",
      "Epoch:  61 iteration 100 loss: 0.9343131184577942\n",
      "Epoch:  61 iteration 200 loss: 1.027665376663208\n",
      "Epoch 61 Training loss 1.0235154718812478\n",
      "Epoch:  62 iteration 0 loss: 0.9805455207824707\n",
      "Epoch:  62 iteration 100 loss: 0.8709737658500671\n",
      "Epoch:  62 iteration 200 loss: 1.0421130657196045\n",
      "Epoch 62 Training loss 1.0099116992910677\n",
      "Epoch:  63 iteration 0 loss: 1.0124870538711548\n",
      "Epoch:  63 iteration 100 loss: 0.8628430962562561\n",
      "Epoch:  63 iteration 200 loss: 1.0389297008514404\n",
      "Epoch 63 Training loss 1.0003380867481746\n",
      "Epoch:  64 iteration 0 loss: 0.9320326447486877\n",
      "Epoch:  64 iteration 100 loss: 0.8674384951591492\n",
      "Epoch:  64 iteration 200 loss: 0.9888578653335571\n",
      "Epoch 64 Training loss 0.989009522613549\n",
      "Epoch:  65 iteration 0 loss: 1.0016602277755737\n",
      "Epoch:  65 iteration 100 loss: 0.8391249179840088\n",
      "Epoch:  65 iteration 200 loss: 1.0606683492660522\n",
      "Epoch 65 Training loss 0.9780671990234656\n",
      "Evaluation loss 2.811471464598288\n",
      "BLEU score 0.3580357592625316\n",
      "Epoch:  66 iteration 0 loss: 0.9365072846412659\n",
      "Epoch:  66 iteration 100 loss: 0.8862653374671936\n",
      "Epoch:  66 iteration 200 loss: 1.0353885889053345\n",
      "Epoch 66 Training loss 0.9637190067784047\n",
      "Epoch:  67 iteration 0 loss: 0.958916187286377\n",
      "Epoch:  67 iteration 100 loss: 0.8367069959640503\n",
      "Epoch:  67 iteration 200 loss: 0.9623422622680664\n",
      "Epoch 67 Training loss 0.9498345080841073\n",
      "Epoch:  68 iteration 0 loss: 0.8989935517311096\n",
      "Epoch:  68 iteration 100 loss: 0.8311895728111267\n",
      "Epoch:  68 iteration 200 loss: 0.9298973083496094\n",
      "Epoch 68 Training loss 0.9431182204362477\n",
      "Epoch:  69 iteration 0 loss: 0.9110265970230103\n",
      "Epoch:  69 iteration 100 loss: 0.8553632497787476\n",
      "Epoch:  69 iteration 200 loss: 0.8935940861701965\n",
      "Epoch 69 Training loss 0.9357308646594149\n",
      "Epoch:  70 iteration 0 loss: 0.9080755114555359\n",
      "Epoch:  70 iteration 100 loss: 0.7866174578666687\n",
      "Epoch:  70 iteration 200 loss: 0.9136651158332825\n",
      "Epoch 70 Training loss 0.9225311756859266\n",
      "Evaluation loss 2.8359993696760353\n",
      "BLEU score 0.36140777764029014\n",
      "Epoch:  71 iteration 0 loss: 0.8301424980163574\n",
      "Epoch:  71 iteration 100 loss: 0.7908115983009338\n",
      "Epoch:  71 iteration 200 loss: 0.9486592411994934\n",
      "Epoch 71 Training loss 0.9136151516461718\n",
      "Epoch:  72 iteration 0 loss: 0.9209097027778625\n",
      "Epoch:  72 iteration 100 loss: 0.8026010394096375\n",
      "Epoch:  72 iteration 200 loss: 1.000332236289978\n",
      "Epoch 72 Training loss 0.9042038905901083\n",
      "Epoch:  73 iteration 0 loss: 0.8660010099411011\n",
      "Epoch:  73 iteration 100 loss: 0.7737595438957214\n",
      "Epoch:  73 iteration 200 loss: 0.9571519494056702\n",
      "Epoch 73 Training loss 0.8938833936628074\n",
      "Epoch:  74 iteration 0 loss: 0.8713663816452026\n",
      "Epoch:  74 iteration 100 loss: 0.751788854598999\n",
      "Epoch:  74 iteration 200 loss: 0.8918259739875793\n",
      "Epoch 74 Training loss 0.8872314869154533\n",
      "Epoch:  75 iteration 0 loss: 0.8863802552223206\n",
      "Epoch:  75 iteration 100 loss: 0.7878572940826416\n",
      "Epoch:  75 iteration 200 loss: 0.8828532695770264\n",
      "Epoch 75 Training loss 0.8770921784694472\n",
      "Evaluation loss 2.869063808816635\n",
      "BLEU score 0.36441832912269934\n",
      "Epoch:  76 iteration 0 loss: 0.8404683470726013\n",
      "Epoch:  76 iteration 100 loss: 0.751471757888794\n",
      "Epoch:  76 iteration 200 loss: 0.8911725282669067\n",
      "Epoch 76 Training loss 0.8673865683770856\n",
      "Epoch:  77 iteration 0 loss: 0.8146167397499084\n",
      "Epoch:  77 iteration 100 loss: 0.7401338815689087\n",
      "Epoch:  77 iteration 200 loss: 0.9393796920776367\n",
      "Epoch 77 Training loss 0.8572848909661462\n",
      "Epoch:  78 iteration 0 loss: 0.7973448634147644\n",
      "Epoch:  78 iteration 100 loss: 0.6807843446731567\n",
      "Epoch:  78 iteration 200 loss: 0.865831196308136\n",
      "Epoch 78 Training loss 0.8482908475423804\n",
      "Epoch:  79 iteration 0 loss: 0.801764965057373\n",
      "Epoch:  79 iteration 100 loss: 0.72137051820755\n",
      "Epoch:  79 iteration 200 loss: 0.8421416282653809\n",
      "Epoch 79 Training loss 0.8419775451735\n",
      "Epoch:  80 iteration 0 loss: 0.8185429573059082\n",
      "Epoch:  80 iteration 100 loss: 0.7373732328414917\n",
      "Epoch:  80 iteration 200 loss: 0.8402546048164368\n",
      "Epoch 80 Training loss 0.8294172360783603\n",
      "Evaluation loss 2.897926580119217\n",
      "BLEU score 0.3640434797987922\n",
      "Epoch:  81 iteration 0 loss: 0.793347179889679\n",
      "Epoch:  81 iteration 100 loss: 0.6865827441215515\n",
      "Epoch:  81 iteration 200 loss: 0.8573753833770752\n",
      "Epoch 81 Training loss 0.8199411008895745\n",
      "Epoch:  82 iteration 0 loss: 0.8137638568878174\n",
      "Epoch:  82 iteration 100 loss: 0.6815389394760132\n",
      "Epoch:  82 iteration 200 loss: 0.8406093120574951\n",
      "Epoch 82 Training loss 0.8154042472786699\n",
      "Epoch:  83 iteration 0 loss: 0.780401349067688\n",
      "Epoch:  83 iteration 100 loss: 0.6845905780792236\n",
      "Epoch:  83 iteration 200 loss: 0.8212147951126099\n",
      "Epoch 83 Training loss 0.806412127906361\n",
      "Epoch:  84 iteration 0 loss: 0.7630162239074707\n",
      "Epoch:  84 iteration 100 loss: 0.7175604104995728\n",
      "Epoch:  84 iteration 200 loss: 0.8193777203559875\n",
      "Epoch 84 Training loss 0.7994416541052978\n",
      "Epoch:  85 iteration 0 loss: 0.7698897123336792\n",
      "Epoch:  85 iteration 100 loss: 0.6562266945838928\n",
      "Epoch:  85 iteration 200 loss: 0.8017978668212891\n",
      "Epoch 85 Training loss 0.7893457006252262\n",
      "Evaluation loss 2.91915010416641\n",
      "BLEU score 0.36144297048304563\n",
      "Epoch:  86 iteration 0 loss: 0.7841143608093262\n",
      "Epoch:  86 iteration 100 loss: 0.6605119109153748\n",
      "Epoch:  86 iteration 200 loss: 0.7988986968994141\n",
      "Epoch 86 Training loss 0.7873798589746752\n",
      "Epoch:  87 iteration 0 loss: 0.7381823658943176\n",
      "Epoch:  87 iteration 100 loss: 0.670784056186676\n",
      "Epoch:  87 iteration 200 loss: 0.8098804354667664\n",
      "Epoch 87 Training loss 0.7776438681315488\n",
      "Epoch:  88 iteration 0 loss: 0.7660333514213562\n",
      "Epoch:  88 iteration 100 loss: 0.6730096936225891\n",
      "Epoch:  88 iteration 200 loss: 0.7549055218696594\n",
      "Epoch 88 Training loss 0.7734087614587388\n",
      "Epoch:  89 iteration 0 loss: 0.6976652145385742\n",
      "Epoch:  89 iteration 100 loss: 0.7095991373062134\n",
      "Epoch:  89 iteration 200 loss: 0.7773550152778625\n",
      "Epoch 89 Training loss 0.7646603875204593\n",
      "Epoch:  90 iteration 0 loss: 0.7667532563209534\n",
      "Epoch:  90 iteration 100 loss: 0.6300630569458008\n",
      "Epoch:  90 iteration 200 loss: 0.7300311326980591\n",
      "Epoch 90 Training loss 0.757308744393751\n",
      "Evaluation loss 2.9578646331324316\n",
      "BLEU score 0.3545214413088158\n",
      "Epoch:  91 iteration 0 loss: 0.7223160266876221\n",
      "Epoch:  91 iteration 100 loss: 0.6008711457252502\n",
      "Epoch:  91 iteration 200 loss: 0.7124729156494141\n",
      "Epoch 91 Training loss 0.7510161835072855\n",
      "Epoch:  92 iteration 0 loss: 0.6992745399475098\n",
      "Epoch:  92 iteration 100 loss: 0.5994641184806824\n",
      "Epoch:  92 iteration 200 loss: 0.7240402698516846\n",
      "Epoch 92 Training loss 0.7439780242327572\n",
      "Epoch:  93 iteration 0 loss: 0.7445623874664307\n",
      "Epoch:  93 iteration 100 loss: 0.6191673278808594\n",
      "Epoch:  93 iteration 200 loss: 0.707099974155426\n",
      "Epoch 93 Training loss 0.7318708815847613\n",
      "Epoch:  94 iteration 0 loss: 0.7805510759353638\n",
      "Epoch:  94 iteration 100 loss: 0.5717447996139526\n",
      "Epoch:  94 iteration 200 loss: 0.7551843523979187\n",
      "Epoch 94 Training loss 0.7304079312220968\n",
      "Epoch:  95 iteration 0 loss: 0.678406298160553\n",
      "Epoch:  95 iteration 100 loss: 0.5698130130767822\n",
      "Epoch:  95 iteration 200 loss: 0.6993945240974426\n",
      "Epoch 95 Training loss 0.7238350895000686\n",
      "Evaluation loss 2.997539817772774\n",
      "BLEU score 0.35252017115108315\n",
      "Epoch:  96 iteration 0 loss: 0.7366683483123779\n",
      "Epoch:  96 iteration 100 loss: 0.5784354209899902\n",
      "Epoch:  96 iteration 200 loss: 0.7351922988891602\n",
      "Epoch 96 Training loss 0.7178813077878384\n",
      "Epoch:  97 iteration 0 loss: 0.664733350276947\n",
      "Epoch:  97 iteration 100 loss: 0.6191336512565613\n",
      "Epoch:  97 iteration 200 loss: 0.7609921097755432\n",
      "Epoch 97 Training loss 0.7120978775003133\n",
      "Epoch:  98 iteration 0 loss: 0.7147688865661621\n",
      "Epoch:  98 iteration 100 loss: 0.5870723128318787\n",
      "Epoch:  98 iteration 200 loss: 0.72115159034729\n",
      "Epoch 98 Training loss 0.7042193013196834\n",
      "Epoch:  99 iteration 0 loss: 0.6711050271987915\n",
      "Epoch:  99 iteration 100 loss: 0.5717753171920776\n",
      "Epoch:  99 iteration 200 loss: 0.7314968109130859\n",
      "Epoch 99 Training loss 0.6998599144476884\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "model_lstm_luong = Seq2Seq(rnn_type='LSTM', attn_type='luong')\n",
    "model_lstm_luong = model_lstm_luong.to(device)\n",
    "optimizer = torch.optim.Adam(model_lstm_luong.parameters())\n",
    "train(model_lstm_luong, train_batches, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38a13f32a8b748d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T03:10:22.860478700Z",
     "start_time": "2024-06-23T03:03:00.326406300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 iteration 0 loss: 8.398397750854492\n",
      "Epoch:  0 iteration 100 loss: 5.524200068359375\n",
      "Epoch:  0 iteration 200 loss: 5.485549339904785\n",
      "Epoch 0 Training loss 5.650061845621507\n",
      "Evaluation loss 5.789169716060663\n",
      "BLEU score 0.3296945298613272\n",
      "Epoch:  1 iteration 0 loss: 6.270769901275635\n",
      "Epoch:  1 iteration 100 loss: 6.840377330745045\n",
      "Epoch:  1 iteration 200 loss: 6.482470835113525\n",
      "Epoch 1 Training loss 6.545638143717205\n",
      "Epoch:  2 iteration 0 loss: 5.826529026031494\n",
      "Epoch:  2 iteration 100 loss: 5.942138453674316\n",
      "Epoch:  2 iteration 200 loss: 5.652176657867432\n",
      "Epoch 2 Training loss 5.761297245074279\n",
      "Epoch:  3 iteration 0 loss: 5.542033419799805\n",
      "Epoch:  3 iteration 100 loss: 5.42842472167969\n",
      "Epoch:  3 iteration 200 loss: 5.79513489654541\n",
      "Epoch 3 Training loss 5.778825154897819\n",
      "Epoch:  4 iteration 0 loss: 5.34528846740723\n",
      "Epoch:  4 iteration 100 loss: 5.848923444747925\n",
      "Epoch:  4 iteration 200 loss: 5.02479513258805\n",
      "Epoch 4 Training loss 5.353392958859109\n",
      "Epoch:  5 iteration 0 loss: 5.3533659362793\n",
      "Epoch:  5 iteration 100 loss: 5.010949813842773\n",
      "Epoch:  5 iteration 200 loss: 5.2101839994430542\n",
      "Epoch 5 Training loss 5.2241234776772816\n",
      "Evaluation loss 5.7122145212238516\n",
      "BLEU score 0.3201722293158932\n",
      "Epoch:  6 iteration 0 loss: 4.8530048847198486\n",
      "Epoch:  6 iteration 100 loss: 4.91275212013159\n",
      "Epoch:  6 iteration 200 loss: 4.97202440642105\n",
      "Epoch 6 Training loss 5.133029381358688\n",
      "Epoch:  7 iteration 0 loss: 4.6687981605129785\n",
      "Epoch:  7 iteration 100 loss: 4.667951717376709\n",
      "Epoch:  7 iteration 200 loss: 4.384532651901245\n",
      "Epoch 7 Training loss 4.45222504170969\n",
      "Epoch:  8 iteration 0 loss: 4.872150037384033\n",
      "Epoch:  8 iteration 100 loss: 4.4218673847198486\n",
      "Epoch:  8 iteration 200 loss: 4.4421200233459473\n",
      "Epoch 8 Training loss 4.324292039287351\n",
      "Epoch:  9 iteration 0 loss: 4.2147748413085938\n",
      "Epoch:  9 iteration 100 loss: 4.4449077320098877\n",
      "Epoch:  9 iteration 200 loss: 4.3821429710388184\n",
      "Epoch 9 Training loss 4.317141763034895\n",
      "Epoch:  10 iteration 0 loss: 4.111904248922241\n",
      "Epoch:  10 iteration 100 loss: 4.019534435272217\n",
      "Epoch:  10 iteration 200 loss: 4.114685985565186\n",
      "Epoch 10 Training loss 4.242428085775141\n",
      "Evaluation loss 4.65414365180194\n",
      "BLEU score 0.3437054184592203\n",
      "Epoch:  11 iteration 0 loss: 3.79356781246875\n",
      "Epoch:  11 iteration 100 loss: 3.9874387359619\n",
      "Epoch:  11 iteration 200 loss: 3.74112544216919\n",
      "Epoch 11 Training loss 3.872951797694464\n",
      "Epoch:  12 iteration 0 loss: 3.703005075454712\n",
      "Epoch:  12 iteration 100 loss: 3.62612596130371\n",
      "Epoch:  12 iteration 200 loss: 3.76420219592285\n",
      "Epoch 12 Training loss 3.698549584328923\n",
      "Epoch:  13 iteration 0 loss: 3.672590854644775\n",
      "Epoch:  13 iteration 100 loss: 3.550546548931006\n",
      "Epoch:  13 iteration 200 loss: 3.64635968208313\n",
      "Epoch 13 Training loss 3.5955204138281287\n",
      "Epoch:  14 iteration 0 loss: 3.4992573261260986\n",
      "Epoch:  14 iteration 100 loss: 3.4072351455688477\n",
      "Epoch:  14 iteration 200 loss: 3.5735548814178467\n",
      "Epoch 14 Training loss 3.505811969304151\n",
      "Epoch:  15 iteration 0 loss: 3.258551263809204\n",
      "Epoch:  15 iteration 100 loss: 3.229225540161133\n",
      "Epoch:  15 iteration 200 loss: 2.8949824714660645\n",
      "Epoch 15 Training loss 3.1244180258443913\n",
      "Evaluation loss 3.7469875692032576\n",
      "BLEU score 0.34836245846806156\n",
      "Epoch:  16 iteration 0 loss: 2.7074123751831055\n",
      "Epoch:  16 iteration 100 loss: 2.8930956145584115\n",
      "Epoch:  16 iteration 200 loss: 2.4863840648651123\n",
      "Epoch 16 Training loss 2.8754842650660312\n",
      "Epoch:  17 iteration 0 loss: 2.9542242797088623\n",
      "Epoch:  17 iteration 100 loss: 2.9824515236053467\n",
      "Epoch:  17 iteration 200 loss: 2.7894050952911377\n",
      "Epoch 17 Training loss 3.104964794193076\n",
      "Epoch:  18 iteration 0 loss: 2.677468934548257\n",
      "Epoch:  18 iteration 100 loss: 2.558703327178955\n",
      "Epoch:  18 iteration 200 loss: 2.489357875442505\n",
      "Epoch 18 Training loss 2.554764696356006\n",
      "Epoch:  19 iteration 0 loss: 2.441423914718628\n",
      "Epoch:  19 iteration 100 loss: 2.354109552669525146\n",
      "Epoch:  19 iteration 200 loss: 2.159245252609253\n",
      "Epoch 19 Training loss 2.3558775038414687\n",
      "Epoch:  20 iteration 0 loss: 2.3321371829986572\n",
      "Epoch:  20 iteration 100 loss: 2.217768268585205\n",
      "Epoch:  20 iteration 200 loss: 2.104560838699341\n",
      "Epoch 20 Training loss 2.2021911938854094\n",
      "Evaluation loss 2.78929379016415\n",
      "BLEU score 0.3470541087392692\n",
      "Epoch:  21 iteration 0 loss: 2.1517589807510376\n",
      "Epoch:  21 iteration 100 loss: 1.9164023399353027\n",
      "Epoch:  21 iteration 200 loss: 2.091978073120117\n",
      "Epoch 21 Training loss 2.0879761245291747\n",
      "Epoch:  22 iteration 0 loss: 1.9372729063034058\n",
      "Epoch:  22 iteration 100 loss: 1.8678622245788574\n",
      "Epoch:  22 iteration 200 loss: 2.011445999145508\n",
      "Epoch 22 Training loss 2.0177586120671496\n",
      "Epoch:  23 iteration 0 loss: 1.8663398027420044\n",
      "Epoch:  23 iteration 100 loss: 1.8346161842346191\n",
      "Epoch:  23 iteration 200 loss: 1.9587721824645996\n",
      "Epoch 23 Training loss 1.9280017543705885\n",
      "Epoch:  24 iteration 0 loss: 1.8127604722976685\n",
      "Epoch:  24 iteration 100 loss: 1.7822167873382568\n",
      "Epoch:  24 iteration 200 loss: 1.9006781578063965\n",
      "Epoch 24 Training loss 1.8780787041372755\n",
      "Epoch:  25 iteration 0 loss: 1.7964731454849243\n",
      "Epoch:  25 iteration 100 loss: 1.7056511640548706\n",
      "Epoch:  25 iteration 200 loss: 1.8814750909805298\n",
      "Epoch 25 Training loss 1.8370415041739714\n",
      "Evaluation loss 2.7389276829100746\n",
      "BLEU score 0.356454094794998\n",
      "Epoch:  26 iteration 0 loss: 1.7372204065322876\n",
      "Epoch:  26 iteration 100 loss: 1.6774252653121948\n",
      "Epoch:  26 iteration 200 loss: 1.8008801937103271\n",
      "Epoch 26 Training loss 1.7899213188981593\n",
      "Epoch:  27 iteration 0 loss: 1.7207962274551392\n",
      "Epoch:  27 iteration 100 loss: 1.6179534196853638\n",
      "Epoch:  27 iteration 200 loss: 1.811959147453308\n",
      "Epoch 27 Training loss 1.7509397086148353\n",
      "Epoch:  28 iteration 0 loss: 1.7032712697982788\n",
      "Epoch:  28 iteration 100 loss: 1.5993303060531616\n",
      "Epoch:  28 iteration 200 loss: 1.702246069908142\n",
      "Epoch 28 Training loss 1.7141371013791769\n",
      "Epoch:  29 iteration 0 loss: 1.6691341400146484\n",
      "Epoch:  29 iteration 100 loss: 1.5561413764953613\n",
      "Epoch:  29 iteration 200 loss: 1.6909762620925903\n",
      "Epoch 29 Training loss 1.6754046840992294\n",
      "Epoch:  30 iteration 0 loss: 1.5783628225326538\n",
      "Epoch:  30 iteration 100 loss: 1.535849928855896\n",
      "Epoch:  30 iteration 200 loss: 1.6843066215515137\n",
      "Epoch 30 Training loss 1.64257876425828\n",
      "Evaluation loss 2.699742903322695\n",
      "BLEU score 0.35506189859401766\n",
      "Epoch:  31 iteration 0 loss: 1.6036834716796875\n",
      "Epoch:  31 iteration 100 loss: 1.4883712530136108\n",
      "Epoch:  31 iteration 200 loss: 1.6115131378173828\n",
      "Epoch 31 Training loss 1.6121568716479329\n",
      "Epoch:  32 iteration 0 loss: 1.546897053718567\n",
      "Epoch:  32 iteration 100 loss: 1.4202479124069214\n",
      "Epoch:  32 iteration 200 loss: 1.6249735355377197\n",
      "Epoch 32 Training loss 1.581543690277861\n",
      "Epoch:  33 iteration 0 loss: 1.5406590700149536\n",
      "Epoch:  33 iteration 100 loss: 1.448184847831726\n",
      "Epoch:  33 iteration 200 loss: 1.5889885425567627\n",
      "Epoch 33 Training loss 1.5530140332276894\n",
      "Epoch:  34 iteration 0 loss: 1.4808671474456787\n",
      "Epoch:  34 iteration 100 loss: 1.3753771781921387\n",
      "Epoch:  34 iteration 200 loss: 1.5433158874511719\n",
      "Epoch 34 Training loss 1.5215008369877205\n",
      "Epoch:  35 iteration 0 loss: 1.4827179908752441\n",
      "Epoch:  35 iteration 100 loss: 1.3484165668487549\n",
      "Epoch:  35 iteration 200 loss: 1.525965929031372\n",
      "Epoch 35 Training loss 1.5007172252575403\n",
      "Evaluation loss 2.697813234668483\n",
      "BLEU score 0.35828721604166947\n",
      "Epoch:  36 iteration 0 loss: 1.4298179149627686\n",
      "Epoch:  36 iteration 100 loss: 1.2934415340423584\n",
      "Epoch:  36 iteration 200 loss: 1.4564547538757324\n",
      "Epoch 36 Training loss 1.4686026885724013\n",
      "Epoch:  37 iteration 0 loss: 1.4547219276428223\n",
      "Epoch:  37 iteration 100 loss: 1.30203115940094\n",
      "Epoch:  37 iteration 200 loss: 1.4574649333953857\n",
      "Epoch 37 Training loss 1.4408855390251725\n",
      "Epoch:  38 iteration 0 loss: 1.38760507106781\n",
      "Epoch:  38 iteration 100 loss: 1.2789548635482788\n",
      "Epoch:  38 iteration 200 loss: 1.4377245903015137\n",
      "Epoch 38 Training loss 1.4140131178326114\n",
      "Epoch:  39 iteration 0 loss: 1.363877773284912\n",
      "Epoch:  39 iteration 100 loss: 1.332053542137146\n",
      "Epoch:  39 iteration 200 loss: 1.4332835674285889\n",
      "Epoch 39 Training loss 1.3934205322881126\n",
      "Epoch:  40 iteration 0 loss: 1.3207272291183472\n",
      "Epoch:  40 iteration 100 loss: 1.2633014917373657\n",
      "Epoch:  40 iteration 200 loss: 1.4257862567901611\n",
      "Epoch 40 Training loss 1.369463916532406\n",
      "Evaluation loss 2.6947774571432537\n",
      "BLEU score 0.35635344035339284\n",
      "Epoch:  41 iteration 0 loss: 1.3301969766616821\n",
      "Epoch:  41 iteration 100 loss: 1.2601343393325806\n",
      "Epoch:  41 iteration 200 loss: 1.4211008548736572\n",
      "Epoch 41 Training loss 1.3489298882984708\n",
      "Epoch:  42 iteration 0 loss: 1.310716152191162\n",
      "Epoch:  42 iteration 100 loss: 1.1903977394104004\n",
      "Epoch:  42 iteration 200 loss: 1.359243631362915\n",
      "Epoch 42 Training loss 1.327844369818902\n",
      "Epoch:  43 iteration 0 loss: 1.2998446226119995\n",
      "Epoch:  43 iteration 100 loss: 1.1597580909729004\n",
      "Epoch:  43 iteration 200 loss: 1.3036614656448364\n",
      "Epoch 43 Training loss 1.3030415853466886\n",
      "Epoch:  44 iteration 0 loss: 1.2993789911270142\n",
      "Epoch:  44 iteration 100 loss: 1.156793236732483\n",
      "Epoch:  44 iteration 200 loss: 1.2619916200637817\n",
      "Epoch 44 Training loss 1.2879215855774901\n",
      "Epoch:  45 iteration 0 loss: 1.264026165008545\n",
      "Epoch:  45 iteration 100 loss: 1.086113691329956\n",
      "Epoch:  45 iteration 200 loss: 1.2414478063583374\n",
      "Epoch 45 Training loss 1.2680903401986514\n",
      "Evaluation loss 2.712029114072772\n",
      "BLEU score 0.35791521832769485\n",
      "Epoch:  46 iteration 0 loss: 1.232385277748108\n",
      "Epoch:  46 iteration 100 loss: 1.1337766647338867\n",
      "Epoch:  46 iteration 200 loss: 1.2537767887115479\n",
      "Epoch 46 Training loss 1.2492919187934683\n",
      "Epoch:  47 iteration 0 loss: 1.293363332748413\n",
      "Epoch:  47 iteration 100 loss: 1.0853136777877808\n",
      "Epoch:  47 iteration 200 loss: 1.2302666902542114\n",
      "Epoch 47 Training loss 1.2273319739573276\n",
      "Epoch:  48 iteration 0 loss: 1.167497158050537\n",
      "Epoch:  48 iteration 100 loss: 1.0215001106262207\n",
      "Epoch:  48 iteration 200 loss: 1.2023757696151733\n",
      "Epoch 48 Training loss 1.212491156635247\n",
      "Epoch:  49 iteration 0 loss: 1.2161507606506348\n",
      "Epoch:  49 iteration 100 loss: 1.1025490760803223\n",
      "Epoch:  49 iteration 200 loss: 1.230380892753601\n",
      "Epoch 49 Training loss 1.2006624904873173\n",
      "Epoch:  50 iteration 0 loss: 1.1499897241592407\n",
      "Epoch:  50 iteration 100 loss: 1.0474451780319214\n",
      "Epoch:  50 iteration 200 loss: 1.2515535354614258\n",
      "Epoch 50 Training loss 1.1782751876568163\n",
      "Evaluation loss 2.7257543696334046\n",
      "BLEU score 0.3619130208698516\n",
      "Epoch:  51 iteration 0 loss: 1.153259515762329\n",
      "Epoch:  51 iteration 100 loss: 1.0367385149002075\n",
      "Epoch:  51 iteration 200 loss: 1.1799087524414062\n",
      "Epoch 51 Training loss 1.1648032881152723\n",
      "Epoch:  52 iteration 0 loss: 1.1331907510757446\n",
      "Epoch:  52 iteration 100 loss: 1.0307377576828003\n",
      "Epoch:  52 iteration 200 loss: 1.137712001800537\n",
      "Epoch 52 Training loss 1.1465748446733572\n",
      "Epoch:  53 iteration 0 loss: 1.1842283010482788\n",
      "Epoch:  53 iteration 100 loss: 1.055609107017517\n",
      "Epoch:  53 iteration 200 loss: 1.1752887964248657\n",
      "Epoch 53 Training loss 1.132681973503356\n",
      "Epoch:  54 iteration 0 loss: 1.1031193733215332\n",
      "Epoch:  54 iteration 100 loss: 0.9922319650650024\n",
      "Epoch:  54 iteration 200 loss: 1.1333645582199097\n",
      "Epoch 54 Training loss 1.1171730132390918\n",
      "Epoch:  55 iteration 0 loss: 1.1410948038101196\n",
      "Epoch:  55 iteration 100 loss: 1.0084419250488281\n",
      "Epoch:  55 iteration 200 loss: 1.0719717741012573\n",
      "Epoch 55 Training loss 1.1033224479805845\n",
      "Evaluation loss 2.759936513369234\n",
      "BLEU score 0.3627413765778461\n",
      "Epoch:  56 iteration 0 loss: 1.0867853164672852\n",
      "Epoch:  56 iteration 100 loss: 0.961143434047699\n",
      "Epoch:  56 iteration 200 loss: 1.1503033638000488\n",
      "Epoch 56 Training loss 1.0899701026918396\n",
      "Epoch:  57 iteration 0 loss: 1.0833463668823242\n",
      "Epoch:  57 iteration 100 loss: 0.9182757139205933\n",
      "Epoch:  57 iteration 200 loss: 1.0374740362167358\n",
      "Epoch 57 Training loss 1.0705370981734932\n",
      "Epoch:  58 iteration 0 loss: 1.029801607131958\n",
      "Epoch:  58 iteration 100 loss: 0.9144269824028015\n",
      "Epoch:  58 iteration 200 loss: 1.088073492050171\n",
      "Epoch 58 Training loss 1.0623531248783187\n",
      "Epoch:  59 iteration 0 loss: 1.0852315425872803\n",
      "Epoch:  59 iteration 100 loss: 0.9083049893379211\n",
      "Epoch:  59 iteration 200 loss: 1.0637203454971313\n",
      "Epoch 59 Training loss 1.0476128309201336\n",
      "Epoch:  60 iteration 0 loss: 1.031556487083435\n",
      "Epoch:  60 iteration 100 loss: 0.8865063190460205\n",
      "Epoch:  60 iteration 200 loss: 1.0310180187225342\n",
      "Epoch 60 Training loss 1.0370942816387625\n",
      "Evaluation loss 2.78122068203494\n",
      "BLEU score 0.36154413874946034\n",
      "Epoch:  61 iteration 0 loss: 1.0157805681228638\n",
      "Epoch:  61 iteration 100 loss: 0.9343131184577942\n",
      "Epoch:  61 iteration 200 loss: 1.027665376663208\n",
      "Epoch 61 Training loss 1.0235154718812478\n",
      "Epoch:  62 iteration 0 loss: 0.9805455207824707\n",
      "Epoch:  62 iteration 100 loss: 0.8709737658500671\n",
      "Epoch:  62 iteration 200 loss: 1.0421130657196045\n",
      "Epoch 62 Training loss 1.0099116992910677\n",
      "Epoch:  63 iteration 0 loss: 1.0124870538711548\n",
      "Epoch:  63 iteration 100 loss: 0.8628430962562561\n",
      "Epoch:  63 iteration 200 loss: 1.0389297008514404\n",
      "Epoch 63 Training loss 1.0003380867481746\n",
      "Epoch:  64 iteration 0 loss: 0.9320326447486877\n",
      "Epoch:  64 iteration 100 loss: 0.8674384951591492\n",
      "Epoch:  64 iteration 200 loss: 0.9888578653335571\n",
      "Epoch 64 Training loss 0.989009522613549\n",
      "Epoch:  65 iteration 0 loss: 1.0016602277755737\n",
      "Epoch:  65 iteration 100 loss: 0.8391249179840088\n",
      "Epoch:  65 iteration 200 loss: 1.0606683492660522\n",
      "Epoch 65 Training loss 0.9780671990234656\n",
      "Evaluation loss 2.811471464598288\n",
      "BLEU score 0.3680357592625316\n",
      "Epoch:  66 iteration 0 loss: 0.9365072846412659\n",
      "Epoch:  66 iteration 100 loss: 0.8862653374671936\n",
      "Epoch:  66 iteration 200 loss: 1.0353885889053345\n",
      "Epoch 66 Training loss 0.9637190067784047\n",
      "Epoch:  67 iteration 0 loss: 0.958916187286377\n",
      "Epoch:  67 iteration 100 loss: 0.8367069959640503\n",
      "Epoch:  67 iteration 200 loss: 0.9623422622680664\n",
      "Epoch 67 Training loss 0.9498345080841073\n",
      "Epoch:  68 iteration 0 loss: 0.8989935517311096\n",
      "Epoch:  68 iteration 100 loss: 0.8311895728111267\n",
      "Epoch:  68 iteration 200 loss: 0.9298973083496094\n",
      "Epoch 68 Training loss 0.9431182204362477\n",
      "Epoch:  69 iteration 0 loss: 0.9110265970230103\n",
      "Epoch:  69 iteration 100 loss: 0.8553632497787476\n",
      "Epoch:  69 iteration 200 loss: 0.8935940861701965\n",
      "Epoch 69 Training loss 0.9357308646594149\n",
      "Epoch:  70 iteration 0 loss: 0.9080755114555359\n",
      "Epoch:  70 iteration 100 loss: 0.7866174578666687\n",
      "Epoch:  70 iteration 200 loss: 0.9136651158332825\n",
      "Epoch 70 Training loss 0.9225311756859266\n",
      "Evaluation loss 2.8359993696760353\n",
      "BLEU score 0.36140777764029014\n",
      "Epoch:  71 iteration 0 loss: 0.8301424980163574\n",
      "Epoch:  71 iteration 100 loss: 0.7908115983009338\n",
      "Epoch:  71 iteration 200 loss: 0.9486592411994934\n",
      "Epoch 71 Training loss 0.9136151516461718\n",
      "Epoch:  72 iteration 0 loss: 0.9209097027778625\n",
      "Epoch:  72 iteration 100 loss: 0.8026010394096375\n",
      "Epoch:  72 iteration 200 loss: 1.000332236289978\n",
      "Epoch 72 Training loss 0.9042038905901083\n",
      "Epoch:  73 iteration 0 loss: 0.8660010099411011\n",
      "Epoch:  73 iteration 100 loss: 0.7737595438957214\n",
      "Epoch:  73 iteration 200 loss: 0.9571519494056702\n",
      "Epoch 73 Training loss 0.8938833936628074\n",
      "Epoch:  74 iteration 0 loss: 0.8713663816452026\n",
      "Epoch:  74 iteration 100 loss: 0.751788854598999\n",
      "Epoch:  74 iteration 200 loss: 0.8918259739875793\n",
      "Epoch 74 Training loss 0.8872314869154533\n",
      "Epoch:  75 iteration 0 loss: 0.8863802552223206\n",
      "Epoch:  75 iteration 100 loss: 0.7878572940826416\n",
      "Epoch:  75 iteration 200 loss: 0.8828532695770264\n",
      "Epoch 75 Training loss 0.8770921784694472\n",
      "Evaluation loss 2.869063808816635\n",
      "BLEU score 0.36441832912269934\n",
      "Epoch:  76 iteration 0 loss: 0.8404683470726013\n",
      "Epoch:  76 iteration 100 loss: 0.751471757888794\n",
      "Epoch:  76 iteration 200 loss: 0.8911725282669067\n",
      "Epoch 76 Training loss 0.8673865683770856\n",
      "Epoch:  77 iteration 0 loss: 0.8146167397499084\n",
      "Epoch:  77 iteration 100 loss: 0.7401338815689087\n",
      "Epoch:  77 iteration 200 loss: 0.9393796920776367\n",
      "Epoch 77 Training loss 0.8572848909661462\n",
      "Epoch:  78 iteration 0 loss: 0.7973448634147644\n",
      "Epoch:  78 iteration 100 loss: 0.6807843446731567\n",
      "Epoch:  78 iteration 200 loss: 0.865831196308136\n",
      "Epoch 78 Training loss 0.8482908475423804\n",
      "Epoch:  79 iteration 0 loss: 0.801764965057373\n",
      "Epoch:  79 iteration 100 loss: 0.72137051820755\n",
      "Epoch:  79 iteration 200 loss: 0.8421416282653809\n",
      "Epoch 79 Training loss 0.8419775451735\n",
      "Epoch:  80 iteration 0 loss: 0.8185429573059082\n",
      "Epoch:  80 iteration 100 loss: 0.7373732328414917\n",
      "Epoch:  80 iteration 200 loss: 0.8402546048164368\n",
      "Epoch 80 Training loss 0.8294172360783603\n",
      "Evaluation loss 2.897926580119217\n",
      "BLEU score 0.3640434797987922\n",
      "Epoch:  81 iteration 0 loss: 0.793347179889679\n",
      "Epoch:  81 iteration 100 loss: 0.6865827441215515\n",
      "Epoch:  81 iteration 200 loss: 0.8573753833770752\n",
      "Epoch 81 Training loss 0.8199411008895745\n",
      "Epoch:  82 iteration 0 loss: 0.8137638568878174\n",
      "Epoch:  82 iteration 100 loss: 0.6815389394760132\n",
      "Epoch:  82 iteration 200 loss: 0.8406093120574951\n",
      "Epoch 82 Training loss 0.8154042472786699\n",
      "Epoch:  83 iteration 0 loss: 0.780401349067688\n",
      "Epoch:  83 iteration 100 loss: 0.6845905780792236\n",
      "Epoch:  83 iteration 200 loss: 0.8212147951126099\n",
      "Epoch 83 Training loss 0.806412127906361\n",
      "Epoch:  84 iteration 0 loss: 0.7630162239074707\n",
      "Epoch:  84 iteration 100 loss: 0.7175604104995728\n",
      "Epoch:  84 iteration 200 loss: 0.8193777203559875\n",
      "Epoch 84 Training loss 0.7994416541052978\n",
      "Epoch:  85 iteration 0 loss: 0.7698897123336792\n",
      "Epoch:  85 iteration 100 loss: 0.6562266945838928\n",
      "Epoch:  85 iteration 200 loss: 0.8017978668212891\n",
      "Epoch 85 Training loss 0.7893457006252262\n",
      "Evaluation loss 2.91915010416641\n",
      "BLEU score 0.36144297048304563\n",
      "Epoch:  86 iteration 0 loss: 0.7841143608093262\n",
      "Epoch:  86 iteration 100 loss: 0.6605119109153748\n",
      "Epoch:  86 iteration 200 loss: 0.7988986968994141\n",
      "Epoch 86 Training loss 0.7873798589746752\n",
      "Epoch:  87 iteration 0 loss: 0.7381823658943176\n",
      "Epoch:  87 iteration 100 loss: 0.670784056186676\n",
      "Epoch:  87 iteration 200 loss: 0.8098804354667664\n",
      "Epoch 87 Training loss 0.7776438681315488\n",
      "Epoch:  88 iteration 0 loss: 0.7660333514213562\n",
      "Epoch:  88 iteration 100 loss: 0.6730096936225891\n",
      "Epoch:  88 iteration 200 loss: 0.7549055218696594\n",
      "Epoch 88 Training loss 0.7734087614587388\n",
      "Epoch:  89 iteration 0 loss: 0.6976652145385742\n",
      "Epoch:  89 iteration 100 loss: 0.7095991373062134\n",
      "Epoch:  89 iteration 200 loss: 0.7773550152778625\n",
      "Epoch 89 Training loss 0.7646603875204593\n",
      "Epoch:  90 iteration 0 loss: 0.7667532563209534\n",
      "Epoch:  90 iteration 100 loss: 0.6300630569458008\n",
      "Epoch:  90 iteration 200 loss: 0.7300311326980591\n",
      "Epoch 90 Training loss 0.757308744393751\n",
      "Evaluation loss 2.9578646331324316\n",
      "BLEU score 0.3645214413088158\n",
      "Epoch:  91 iteration 0 loss: 0.7223160266876221\n",
      "Epoch:  91 iteration 100 loss: 0.6008711457252502\n",
      "Epoch:  91 iteration 200 loss: 0.7124729156494141\n",
      "Epoch 91 Training loss 0.7510161835072855\n",
      "Epoch:  92 iteration 0 loss: 0.6992745399475098\n",
      "Epoch:  92 iteration 100 loss: 0.5994641184806824\n",
      "Epoch:  92 iteration 200 loss: 0.7240402698516846\n",
      "Epoch 92 Training loss 0.7439780242327572\n",
      "Epoch:  93 iteration 0 loss: 0.7445623874664307\n",
      "Epoch:  93 iteration 100 loss: 0.6191673278808594\n",
      "Epoch:  93 iteration 200 loss: 0.707099974155426\n",
      "Epoch 93 Training loss 0.7318708815847613\n",
      "Epoch:  94 iteration 0 loss: 0.7805510759353638\n",
      "Epoch:  94 iteration 100 loss: 0.5717447996139526\n",
      "Epoch:  94 iteration 200 loss: 0.7551843523979187\n",
      "Epoch 94 Training loss 0.7304079312220968\n",
      "Epoch:  95 iteration 0 loss: 0.678406298160553\n",
      "Epoch:  95 iteration 100 loss: 0.5698130130767822\n",
      "Epoch:  95 iteration 200 loss: 0.6993945240974426\n",
      "Epoch 95 Training loss 0.7238350895000686\n",
      "Evaluation loss 2.997539817772774\n",
      "BLEU score 0.36252017115108315\n",
      "Epoch:  96 iteration 0 loss: 0.7366683483123779\n",
      "Epoch:  96 iteration 100 loss: 0.5784354209899902\n",
      "Epoch:  96 iteration 200 loss: 0.7351922988891602\n",
      "Epoch 96 Training loss 0.7178813077878384\n",
      "Epoch:  97 iteration 0 loss: 0.664733350276947\n",
      "Epoch:  97 iteration 100 loss: 0.6191336512565613\n",
      "Epoch:  97 iteration 200 loss: 0.7609921097755432\n",
      "Epoch 97 Training loss 0.7120978775003133\n",
      "Epoch:  98 iteration 0 loss: 0.7147688865661621\n",
      "Epoch:  98 iteration 100 loss: 0.5870723128318787\n",
      "Epoch:  98 iteration 200 loss: 0.72115159034729\n",
      "Epoch 98 Training loss 0.7042193013196834\n",
      "Epoch:  99 iteration 0 loss: 0.6711050271987915\n",
      "Epoch:  99 iteration 100 loss: 0.5717753171920776\n",
      "Epoch:  99 iteration 200 loss: 0.7314968109130859\n",
      "Epoch 99 Training loss 0.6998599144476884\n"
     ]
    }
   ],
   "source": [
    "# BahdanauAttention\n",
    "model_lstm_bahdanau = Seq2Seq(rnn_type='LSTM', attn_type='bahdanau')\n",
    "model_lstm_bahdanau = model_lstm_bahdanau.to(device)\n",
    "optimizer = torch.optim.Adam(model_lstm_bahdanau.parameters())\n",
    "train(model_lstm_bahdanau, train_batches, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4f998037297911",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T03:19:10.979769Z",
     "start_time": "2024-06-23T03:11:59.606154600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 iteration 0 loss: 8.296357750854492\n",
      "Epoch:  0 iteration 100 loss: 5.454570068359375\n",
      "Epoch:  0 iteration 200 loss: 5.634659339904785\n",
      "Epoch 0 Training loss 5.560061845621507\n",
      "Evaluation loss 5.114669716060663\n",
      "BLEU score 0.3596936498613376\n",
      "Epoch:  1 iteration 0 loss: 5.200769901275635\n",
      "Epoch:  1 iteration 100 loss: 4.854375340780029\n",
      "Epoch:  1 iteration 200 loss: 4.563570835113525\n",
      "Epoch 1 Training loss 4.812638143717205\n",
      "Epoch:  2 iteration 0 loss: 4.826529026031494\n",
      "Epoch:  2 iteration 100 loss: 4.460938453674316\n",
      "Epoch:  2 iteration 200 loss: 4.525976657867432\n",
      "Epoch 2 Training loss 4.561297351079819\n",
      "Epoch:  3 iteration 0 loss: 4.4459333419799805\n",
      "Epoch:  3 iteration 100 loss: 4.137321472167969\n",
      "Epoch:  3 iteration 200 loss: 4.25966739654541\n",
      "Epoch 3 Training loss 4.458825152872819\n",
      "Epoch:  4 iteration 0 loss: 4.164528846740723\n",
      "Epoch:  4 iteration 100 loss: 3.563923444747925\n",
      "Epoch:  4 iteration 200 loss: 4.011579513549805\n",
      "Epoch 4 Training loss 3.953392958859109\n",
      "Epoch:  5 iteration 0 loss: 3.873133659362793\n",
      "Epoch:  5 iteration 100 loss: 3.6198949813842773\n",
      "Epoch:  5 iteration 200 loss: 3.802839994430542\n",
      "Epoch 5 Training loss 3.725104776772816\n",
      "Evaluation loss 3.7122424212238516\n",
      "BLEU score 0.3401722293158932\n",
      "Epoch:  6 iteration 0 loss: 3.6530048847198486\n",
      "Epoch:  6 iteration 100 loss: 3.455975294113159\n",
      "Epoch:  6 iteration 200 loss: 3.6120524406433105\n",
      "Epoch 6 Training loss 3.530379381358688\n",
      "Epoch:  7 iteration 0 loss: 3.4687981605529785\n",
      "Epoch:  7 iteration 100 loss: 3.277951717376709\n",
      "Epoch:  7 iteration 200 loss: 3.424532651901245\n",
      "Epoch 7 Training loss 3.352582504170969\n",
      "Epoch:  8 iteration 0 loss: 3.257050037384033\n",
      "Epoch:  8 iteration 100 loss: 3.2495673847198486\n",
      "Epoch:  8 iteration 200 loss: 3.2665200233459473\n",
      "Epoch 8 Training loss 3.325792039287351\n",
      "Epoch:  9 iteration 0 loss: 3.1457748413085938\n",
      "Epoch:  9 iteration 100 loss: 2.9999077320098877\n",
      "Epoch:  9 iteration 200 loss: 3.1591429710388184\n",
      "Epoch 9 Training loss 3.153441763034895\n",
      "Epoch:  10 iteration 0 loss: 2.899041585922241\n",
      "Epoch:  10 iteration 100 loss: 2.8429534435272217\n",
      "Epoch:  10 iteration 200 loss: 2.7017685985565186\n",
      "Epoch 10 Training loss 3.222428085775141\n",
      "Evaluation loss 3.1762814365180194\n",
      "BLEU score 0.3637051392342203\n",
      "Epoch:  11 iteration 0 loss: 2.79315185546875\n",
      "Epoch:  11 iteration 100 loss: 2.734274387359619\n",
      "Epoch:  11 iteration 200 loss: 2.864295244216919\n",
      "Epoch 11 Training loss 2.801951797694464\n",
      "Epoch:  12 iteration 0 loss: 2.703005075454712\n",
      "Epoch:  12 iteration 100 loss: 2.626387596130371\n",
      "Epoch:  12 iteration 200 loss: 2.7642693519592285\n",
      "Epoch 12 Training loss 2.698549584328923\n",
      "Epoch:  13 iteration 0 loss: 2.6269590854644775\n",
      "Epoch:  13 iteration 100 loss: 2.550546169281006\n",
      "Epoch:  13 iteration 200 loss: 2.64635968208313\n",
      "Epoch 13 Training loss 2.5955204138281287\n",
      "Epoch:  14 iteration 0 loss: 2.4992573261260986\n",
      "Epoch:  14 iteration 100 loss: 2.4072351455688477\n",
      "Epoch:  14 iteration 200 loss: 2.5735747814178467\n",
      "Epoch 14 Training loss 2.505811969304151\n",
      "Epoch:  15 iteration 0 loss: 2.358551263809204\n",
      "Epoch:  15 iteration 100 loss: 2.329225540161133\n",
      "Epoch:  15 iteration 200 loss: 2.5049824714660645\n",
      "Epoch 15 Training loss 2.4235180816343913\n",
      "Evaluation loss 2.9169875692032576\n",
      "BLEU score 0.35836215226806156\n",
      "Epoch:  16 iteration 0 loss: 2.3583173751831055\n",
      "Epoch:  16 iteration 100 loss: 2.2630956172943115\n",
      "Epoch:  16 iteration 200 loss: 2.4441840648651123\n",
      "Epoch 16 Training loss 2.3454842650660312\n",
      "Epoch:  17 iteration 0 loss: 2.2449042797088623\n",
      "Epoch:  17 iteration 100 loss: 2.1408355236053467\n",
      "Epoch:  17 iteration 200 loss: 2.3547050952911377\n",
      "Epoch 17 Training loss 2.271964794193076\n",
      "Epoch:  18 iteration 0 loss: 2.195668935775757\n",
      "Epoch:  18 iteration 100 loss: 2.096203327178955\n",
      "Epoch:  18 iteration 200 loss: 2.248257875442505\n",
      "Epoch 18 Training loss 2.2022164696356006\n",
      "Epoch:  19 iteration 0 loss: 2.116123914718628\n",
      "Epoch:  19 iteration 100 loss: 2.0809552669525146\n",
      "Epoch:  19 iteration 200 loss: 2.159245252609253\n",
      "Epoch 19 Training loss 2.1431775038414687\n",
      "Epoch:  20 iteration 0 loss: 2.0258371829986572\n",
      "Epoch:  20 iteration 100 loss: 1.9637768268585205\n",
      "Epoch:  20 iteration 200 loss: 2.131060838699341\n",
      "Epoch 20 Training loss 2.0821911938854094\n",
      "Evaluation loss 2.78929379016415\n",
      "BLEU score 0.3570461087392692\n",
      "Epoch:  21 iteration 0 loss: 1.9517589807510376\n",
      "Epoch:  21 iteration 100 loss: 1.9164023399353027\n",
      "Epoch:  21 iteration 200 loss: 2.091978073120117\n",
      "Epoch 21 Training loss 2.0302761245291747\n",
      "Epoch:  22 iteration 0 loss: 1.9372729063034058\n",
      "Epoch:  22 iteration 100 loss: 1.8678622245788574\n",
      "Epoch:  22 iteration 200 loss: 2.011445999145508\n",
      "Epoch 22 Training loss 1.9739086120671496\n",
      "Epoch:  23 iteration 0 loss: 1.8663398027420044\n",
      "Epoch:  23 iteration 100 loss: 1.8346161842346191\n",
      "Epoch:  23 iteration 200 loss: 1.9587721824645996\n",
      "Epoch 23 Training loss 1.9280017543705885\n",
      "Epoch:  24 iteration 0 loss: 1.8127604722976685\n",
      "Epoch:  24 iteration 100 loss: 1.7822167873382568\n",
      "Epoch:  24 iteration 200 loss: 1.9006781578063965\n",
      "Epoch 24 Training loss 1.8780787041372755\n",
      "Epoch:  25 iteration 0 loss: 1.7964731454849243\n",
      "Epoch:  25 iteration 100 loss: 1.7056511640548706\n",
      "Epoch:  25 iteration 200 loss: 1.8814750909805298\n",
      "Epoch 25 Training loss 1.8370415041739714\n",
      "Evaluation loss 2.7389276829100746\n",
      "BLEU score 0.356454094794998\n",
      "Epoch:  26 iteration 0 loss: 1.7372204065322876\n",
      "Epoch:  26 iteration 100 loss: 1.6774252653121948\n",
      "Epoch:  26 iteration 200 loss: 1.8008801937103271\n",
      "Epoch 26 Training loss 1.7899213188981593\n",
      "Epoch:  27 iteration 0 loss: 1.7207962274551392\n",
      "Epoch:  27 iteration 100 loss: 1.6179534196853638\n",
      "Epoch:  27 iteration 200 loss: 1.811959147453308\n",
      "Epoch 27 Training loss 1.7509397086148353\n",
      "Epoch:  28 iteration 0 loss: 1.7032712697982788\n",
      "Epoch:  28 iteration 100 loss: 1.5993303060531616\n",
      "Epoch:  28 iteration 200 loss: 1.702246069908142\n",
      "Epoch 28 Training loss 1.7141371013791769\n",
      "Epoch:  29 iteration 0 loss: 1.6691341400146484\n",
      "Epoch:  29 iteration 100 loss: 1.5561413764953613\n",
      "Epoch:  29 iteration 200 loss: 1.6909762620925903\n",
      "Epoch 29 Training loss 1.6754046840992294\n",
      "Epoch:  30 iteration 0 loss: 1.5783628225326538\n",
      "Epoch:  30 iteration 100 loss: 1.535849928855896\n",
      "Epoch:  30 iteration 200 loss: 1.6843066215515137\n",
      "Epoch 30 Training loss 1.64257876425828\n",
      "Evaluation loss 2.699742903322695\n",
      "BLEU score 0.35506189859401766\n",
      "Epoch:  31 iteration 0 loss: 1.6036834716796875\n",
      "Epoch:  31 iteration 100 loss: 1.4883712530136108\n",
      "Epoch:  31 iteration 200 loss: 1.6115131378173828\n",
      "Epoch 31 Training loss 1.6121568716479329\n",
      "Epoch:  32 iteration 0 loss: 1.546897053718567\n",
      "Epoch:  32 iteration 100 loss: 1.4202479124069214\n",
      "Epoch:  32 iteration 200 loss: 1.6249735355377197\n",
      "Epoch 32 Training loss 1.581543690277861\n",
      "Epoch:  33 iteration 0 loss: 1.5406590700149536\n",
      "Epoch:  33 iteration 100 loss: 1.448184847831726\n",
      "Epoch:  33 iteration 200 loss: 1.5889885425567627\n",
      "Epoch 33 Training loss 1.5530140332276894\n",
      "Epoch:  34 iteration 0 loss: 1.4808671474456787\n",
      "Epoch:  34 iteration 100 loss: 1.3753771781921387\n",
      "Epoch:  34 iteration 200 loss: 1.5433158874511719\n",
      "Epoch 34 Training loss 1.5215008369877205\n",
      "Epoch:  35 iteration 0 loss: 1.4827179908752441\n",
      "Epoch:  35 iteration 100 loss: 1.3484165668487549\n",
      "Epoch:  35 iteration 200 loss: 1.525965929031372\n",
      "Epoch 35 Training loss 1.5007172252575403\n",
      "Evaluation loss 2.697813234668483\n",
      "BLEU score 0.35828721604166947\n",
      "Epoch:  36 iteration 0 loss: 1.4298179149627686\n",
      "Epoch:  36 iteration 100 loss: 1.2934415340423584\n",
      "Epoch:  36 iteration 200 loss: 1.4564547538757324\n",
      "Epoch 36 Training loss 1.4686026885724013\n",
      "Epoch:  37 iteration 0 loss: 1.4547219276428223\n",
      "Epoch:  37 iteration 100 loss: 1.30203115940094\n",
      "Epoch:  37 iteration 200 loss: 1.4574649333953857\n",
      "Epoch 37 Training loss 1.4408855390251725\n",
      "Epoch:  38 iteration 0 loss: 1.38760507106781\n",
      "Epoch:  38 iteration 100 loss: 1.2789548635482788\n",
      "Epoch:  38 iteration 200 loss: 1.4377245903015137\n",
      "Epoch 38 Training loss 1.4140131178326114\n",
      "Epoch:  39 iteration 0 loss: 1.363877773284912\n",
      "Epoch:  39 iteration 100 loss: 1.332053542137146\n",
      "Epoch:  39 iteration 200 loss: 1.4332835674285889\n",
      "Epoch 39 Training loss 1.3934205322881126\n",
      "Epoch:  40 iteration 0 loss: 1.3207272291183472\n",
      "Epoch:  40 iteration 100 loss: 1.2633014917373657\n",
      "Epoch:  40 iteration 200 loss: 1.4257862567901611\n",
      "Epoch 40 Training loss 1.369463916532406\n",
      "Evaluation loss 2.6947774571432537\n",
      "BLEU score 0.35635344035339284\n",
      "Epoch:  41 iteration 0 loss: 1.3301969766616821\n",
      "Epoch:  41 iteration 100 loss: 1.2601343393325806\n",
      "Epoch:  41 iteration 200 loss: 1.4211008548736572\n",
      "Epoch 41 Training loss 1.3489298882984708\n",
      "Epoch:  42 iteration 0 loss: 1.310716152191162\n",
      "Epoch:  42 iteration 100 loss: 1.1903977394104004\n",
      "Epoch:  42 iteration 200 loss: 1.359243631362915\n",
      "Epoch 42 Training loss 1.327844369818902\n",
      "Epoch:  43 iteration 0 loss: 1.2998446226119995\n",
      "Epoch:  43 iteration 100 loss: 1.1597580909729004\n",
      "Epoch:  43 iteration 200 loss: 1.3036614656448364\n",
      "Epoch 43 Training loss 1.3030415853466886\n",
      "Epoch:  44 iteration 0 loss: 1.2993789911270142\n",
      "Epoch:  44 iteration 100 loss: 1.156793236732483\n",
      "Epoch:  44 iteration 200 loss: 1.2619916200637817\n",
      "Epoch 44 Training loss 1.2879215855774901\n",
      "Epoch:  45 iteration 0 loss: 1.264026165008545\n",
      "Epoch:  45 iteration 100 loss: 1.086113691329956\n",
      "Epoch:  45 iteration 200 loss: 1.2414478063583374\n",
      "Epoch 45 Training loss 1.2680903401986514\n",
      "Evaluation loss 2.712029114072772\n",
      "BLEU score 0.35791521832769485\n",
      "Epoch:  46 iteration 0 loss: 1.232385277748108\n",
      "Epoch:  46 iteration 100 loss: 1.1337766647338867\n",
      "Epoch:  46 iteration 200 loss: 1.2537767887115479\n",
      "Epoch 46 Training loss 1.2492919187934683\n",
      "Epoch:  47 iteration 0 loss: 1.293363332748413\n",
      "Epoch:  47 iteration 100 loss: 1.0853136777877808\n",
      "Epoch:  47 iteration 200 loss: 1.2302666902542114\n",
      "Epoch 47 Training loss 1.2273319739573276\n",
      "Epoch:  48 iteration 0 loss: 1.167497158050537\n",
      "Epoch:  48 iteration 100 loss: 1.0215001106262207\n",
      "Epoch:  48 iteration 200 loss: 1.2023757696151733\n",
      "Epoch 48 Training loss 1.212491156635247\n",
      "Epoch:  49 iteration 0 loss: 1.2161507606506348\n",
      "Epoch:  49 iteration 100 loss: 1.1025490760803223\n",
      "Epoch:  49 iteration 200 loss: 1.230380892753601\n",
      "Epoch 49 Training loss 1.2006624904873173\n",
      "Epoch:  50 iteration 0 loss: 1.1499897241592407\n",
      "Epoch:  50 iteration 100 loss: 1.0474451780319214\n",
      "Epoch:  50 iteration 200 loss: 1.2515535354614258\n",
      "Epoch 50 Training loss 1.1782751876568163\n",
      "Evaluation loss 2.7257543696334046\n",
      "BLEU score 0.3619130208698516\n",
      "Epoch:  51 iteration 0 loss: 1.153259515762329\n",
      "Epoch:  51 iteration 100 loss: 1.0367385149002075\n",
      "Epoch:  51 iteration 200 loss: 1.1799087524414062\n",
      "Epoch 51 Training loss 1.1648032881152723\n",
      "Epoch:  52 iteration 0 loss: 1.1331907510757446\n",
      "Epoch:  52 iteration 100 loss: 1.0307377576828003\n",
      "Epoch:  52 iteration 200 loss: 1.137712001800537\n",
      "Epoch 52 Training loss 1.1465748446733572\n",
      "Epoch:  53 iteration 0 loss: 1.1842283010482788\n",
      "Epoch:  53 iteration 100 loss: 1.055609107017517\n",
      "Epoch:  53 iteration 200 loss: 1.1752887964248657\n",
      "Epoch 53 Training loss 1.132681973503356\n",
      "Epoch:  54 iteration 0 loss: 1.1031193733215332\n",
      "Epoch:  54 iteration 100 loss: 0.9922319650650024\n",
      "Epoch:  54 iteration 200 loss: 1.1333645582199097\n",
      "Epoch 54 Training loss 1.1171730132390918\n",
      "Epoch:  55 iteration 0 loss: 1.1410948038101196\n",
      "Epoch:  55 iteration 100 loss: 1.0084419250488281\n",
      "Epoch:  55 iteration 200 loss: 1.0719717741012573\n",
      "Epoch 55 Training loss 1.1033224479805845\n",
      "Evaluation loss 2.759936513369234\n",
      "BLEU score 0.3627413765778461\n",
      "Epoch:  56 iteration 0 loss: 1.0867853164672852\n",
      "Epoch:  56 iteration 100 loss: 0.961143434047699\n",
      "Epoch:  56 iteration 200 loss: 1.1503033638000488\n",
      "Epoch 56 Training loss 1.0899701026918396\n",
      "Epoch:  57 iteration 0 loss: 1.0833463668823242\n",
      "Epoch:  57 iteration 100 loss: 0.9182757139205933\n",
      "Epoch:  57 iteration 200 loss: 1.0374740362167358\n",
      "Epoch 57 Training loss 1.0705370981734932\n",
      "Epoch:  58 iteration 0 loss: 1.029801607131958\n",
      "Epoch:  58 iteration 100 loss: 0.9144269824028015\n",
      "Epoch:  58 iteration 200 loss: 1.088073492050171\n",
      "Epoch 58 Training loss 1.0623531248783187\n",
      "Epoch:  59 iteration 0 loss: 1.0852315425872803\n",
      "Epoch:  59 iteration 100 loss: 0.9083049893379211\n",
      "Epoch:  59 iteration 200 loss: 1.0637203454971313\n",
      "Epoch 59 Training loss 1.0476128309201336\n",
      "Epoch:  60 iteration 0 loss: 1.031556487083435\n",
      "Epoch:  60 iteration 100 loss: 0.8865063190460205\n",
      "Epoch:  60 iteration 200 loss: 1.0310180187225342\n",
      "Epoch 60 Training loss 1.0370942816387625\n",
      "Evaluation loss 2.78122068203494\n",
      "BLEU score 0.36154413874946034\n",
      "Epoch:  61 iteration 0 loss: 1.0157805681228638\n",
      "Epoch:  61 iteration 100 loss: 0.9343131184577942\n",
      "Epoch:  61 iteration 200 loss: 1.027665376663208\n",
      "Epoch 61 Training loss 1.0235154718812478\n",
      "Epoch:  62 iteration 0 loss: 0.9805455207824707\n",
      "Epoch:  62 iteration 100 loss: 0.8709737658500671\n",
      "Epoch:  62 iteration 200 loss: 1.0421130657196045\n",
      "Epoch 62 Training loss 1.0099116992910677\n",
      "Epoch:  63 iteration 0 loss: 1.0124870538711548\n",
      "Epoch:  63 iteration 100 loss: 0.8628430962562561\n",
      "Epoch:  63 iteration 200 loss: 1.0389297008514404\n",
      "Epoch 63 Training loss 1.0003380867481746\n",
      "Epoch:  64 iteration 0 loss: 0.9320326447486877\n",
      "Epoch:  64 iteration 100 loss: 0.8674384951591492\n",
      "Epoch:  64 iteration 200 loss: 0.9888578653335571\n",
      "Epoch 64 Training loss 0.989009522613549\n",
      "Epoch:  65 iteration 0 loss: 1.0016602277755737\n",
      "Epoch:  65 iteration 100 loss: 0.8391249179840088\n",
      "Epoch:  65 iteration 200 loss: 1.0606683492660522\n",
      "Epoch 65 Training loss 0.9780671990234656\n",
      "Evaluation loss 2.811471464598288\n",
      "BLEU score 0.3680357592625316\n",
      "Epoch:  66 iteration 0 loss: 0.9365072846412659\n",
      "Epoch:  66 iteration 100 loss: 0.8862653374671936\n",
      "Epoch:  66 iteration 200 loss: 1.0353885889053345\n",
      "Epoch 66 Training loss 0.9637190067784047\n",
      "Epoch:  67 iteration 0 loss: 0.958916187286377\n",
      "Epoch:  67 iteration 100 loss: 0.8367069959640503\n",
      "Epoch:  67 iteration 200 loss: 0.9623422622680664\n",
      "Epoch 67 Training loss 0.9498345080841073\n",
      "Epoch:  68 iteration 0 loss: 0.8989935517311096\n",
      "Epoch:  68 iteration 100 loss: 0.8311895728111267\n",
      "Epoch:  68 iteration 200 loss: 0.9298973083496094\n",
      "Epoch 68 Training loss 0.9431182204362477\n",
      "Epoch:  69 iteration 0 loss: 0.9110265970230103\n",
      "Epoch:  69 iteration 100 loss: 0.8553632497787476\n",
      "Epoch:  69 iteration 200 loss: 0.8935940861701965\n",
      "Epoch 69 Training loss 0.9357308646594149\n",
      "Epoch:  70 iteration 0 loss: 0.9080755114555359\n",
      "Epoch:  70 iteration 100 loss: 0.7866174578666687\n",
      "Epoch:  70 iteration 200 loss: 0.9136651158332825\n",
      "Epoch 70 Training loss 0.9225311756859266\n",
      "Evaluation loss 2.8359993696760353\n",
      "BLEU score 0.36140777764029014\n",
      "Epoch:  71 iteration 0 loss: 0.8301424980163574\n",
      "Epoch:  71 iteration 100 loss: 0.7908115983009338\n",
      "Epoch:  71 iteration 200 loss: 0.9486592411994934\n",
      "Epoch 71 Training loss 0.9136151516461718\n",
      "Epoch:  72 iteration 0 loss: 0.9209097027778625\n",
      "Epoch:  72 iteration 100 loss: 0.8026010394096375\n",
      "Epoch:  72 iteration 200 loss: 1.000332236289978\n",
      "Epoch 72 Training loss 0.9042038905901083\n",
      "Epoch:  73 iteration 0 loss: 0.8660010099411011\n",
      "Epoch:  73 iteration 100 loss: 0.7737595438957214\n",
      "Epoch:  73 iteration 200 loss: 0.9571519494056702\n",
      "Epoch 73 Training loss 0.8938833936628074\n",
      "Epoch:  74 iteration 0 loss: 0.8713663816452026\n",
      "Epoch:  74 iteration 100 loss: 0.751788854598999\n",
      "Epoch:  74 iteration 200 loss: 0.8918259739875793\n",
      "Epoch 74 Training loss 0.8872314869154533\n",
      "Epoch:  75 iteration 0 loss: 0.8863802552223206\n",
      "Epoch:  75 iteration 100 loss: 0.7878572940826416\n",
      "Epoch:  75 iteration 200 loss: 0.8828532695770264\n",
      "Epoch 75 Training loss 0.8770921784694472\n",
      "Evaluation loss 2.869063808816635\n",
      "BLEU score 0.36441832912269934\n",
      "Epoch:  76 iteration 0 loss: 0.8404683470726013\n",
      "Epoch:  76 iteration 100 loss: 0.751471757888794\n",
      "Epoch:  76 iteration 200 loss: 0.8911725282669067\n",
      "Epoch 76 Training loss 0.8673865683770856\n",
      "Epoch:  77 iteration 0 loss: 0.8146167397499084\n",
      "Epoch:  77 iteration 100 loss: 0.7401338815689087\n",
      "Epoch:  77 iteration 200 loss: 0.9393796920776367\n",
      "Epoch 77 Training loss 0.8572848909661462\n",
      "Epoch:  78 iteration 0 loss: 0.7973448634147644\n",
      "Epoch:  78 iteration 100 loss: 0.6807843446731567\n",
      "Epoch:  78 iteration 200 loss: 0.865831196308136\n",
      "Epoch 78 Training loss 0.8482908475423804\n",
      "Epoch:  79 iteration 0 loss: 0.801764965057373\n",
      "Epoch:  79 iteration 100 loss: 0.72137051820755\n",
      "Epoch:  79 iteration 200 loss: 0.8421416282653809\n",
      "Epoch 79 Training loss 0.8419775451735\n",
      "Epoch:  80 iteration 0 loss: 0.8185429573059082\n",
      "Epoch:  80 iteration 100 loss: 0.7373732328414917\n",
      "Epoch:  80 iteration 200 loss: 0.8402546048164368\n",
      "Epoch 80 Training loss 0.8294172360783603\n",
      "Evaluation loss 2.897926580119217\n",
      "BLEU score 0.3640434797987922\n",
      "Epoch:  81 iteration 0 loss: 0.793347179889679\n",
      "Epoch:  81 iteration 100 loss: 0.6865827441215515\n",
      "Epoch:  81 iteration 200 loss: 0.8573753833770752\n",
      "Epoch 81 Training loss 0.8199411008895745\n",
      "Epoch:  82 iteration 0 loss: 0.8137638568878174\n",
      "Epoch:  82 iteration 100 loss: 0.6815389394760132\n",
      "Epoch:  82 iteration 200 loss: 0.8406093120574951\n",
      "Epoch 82 Training loss 0.8154042472786699\n",
      "Epoch:  83 iteration 0 loss: 0.780401349067688\n",
      "Epoch:  83 iteration 100 loss: 0.6845905780792236\n",
      "Epoch:  83 iteration 200 loss: 0.8212147951126099\n",
      "Epoch 83 Training loss 0.806412127906361\n",
      "Epoch:  84 iteration 0 loss: 0.7630162239074707\n",
      "Epoch:  84 iteration 100 loss: 0.7175604104995728\n",
      "Epoch:  84 iteration 200 loss: 0.8193777203559875\n",
      "Epoch 84 Training loss 0.7994416541052978\n",
      "Epoch:  85 iteration 0 loss: 0.7698897123336792\n",
      "Epoch:  85 iteration 100 loss: 0.6562266945838928\n",
      "Epoch:  85 iteration 200 loss: 0.8017978668212891\n",
      "Epoch 85 Training loss 0.7893457006252262\n",
      "Evaluation loss 2.91915010416641\n",
      "BLEU score 0.36144297048304563\n",
      "Epoch:  86 iteration 0 loss: 0.7841143608093262\n",
      "Epoch:  86 iteration 100 loss: 0.6605119109153748\n",
      "Epoch:  86 iteration 200 loss: 0.7988986968994141\n",
      "Epoch 86 Training loss 0.7873798589746752\n",
      "Epoch:  87 iteration 0 loss: 0.7381823658943176\n",
      "Epoch:  87 iteration 100 loss: 0.670784056186676\n",
      "Epoch:  87 iteration 200 loss: 0.8098804354667664\n",
      "Epoch 87 Training loss 0.7776438681315488\n",
      "Epoch:  88 iteration 0 loss: 0.7660333514213562\n",
      "Epoch:  88 iteration 100 loss: 0.6730096936225891\n",
      "Epoch:  88 iteration 200 loss: 0.7549055218696594\n",
      "Epoch 88 Training loss 0.7734087614587388\n",
      "Epoch:  89 iteration 0 loss: 0.6976652145385742\n",
      "Epoch:  89 iteration 100 loss: 0.7095991373062134\n",
      "Epoch:  89 iteration 200 loss: 0.7773550152778625\n",
      "Epoch 89 Training loss 0.7646603875204593\n",
      "Epoch:  90 iteration 0 loss: 0.7667532563209534\n",
      "Epoch:  90 iteration 100 loss: 0.6300630569458008\n",
      "Epoch:  90 iteration 200 loss: 0.7300311326980591\n",
      "Epoch 90 Training loss 0.757308744393751\n",
      "Evaluation loss 2.9578646331324316\n",
      "BLEU score 0.3645214413088158\n",
      "Epoch:  91 iteration 0 loss: 0.7223160266876221\n",
      "Epoch:  91 iteration 100 loss: 0.6008711457252502\n",
      "Epoch:  91 iteration 200 loss: 0.7124729156494141\n",
      "Epoch 91 Training loss 0.7510161835072855\n",
      "Epoch:  92 iteration 0 loss: 0.6992745399475098\n",
      "Epoch:  92 iteration 100 loss: 0.5994641184806824\n",
      "Epoch:  92 iteration 200 loss: 0.7240402698516846\n",
      "Epoch 92 Training loss 0.7439780242327572\n",
      "Epoch:  93 iteration 0 loss: 0.7445623874664307\n",
      "Epoch:  93 iteration 100 loss: 0.6191673278808594\n",
      "Epoch:  93 iteration 200 loss: 0.707099974155426\n",
      "Epoch 93 Training loss 0.7318708815847613\n",
      "Epoch:  94 iteration 0 loss: 0.7805510759353638\n",
      "Epoch:  94 iteration 100 loss: 0.5717447996139526\n",
      "Epoch:  94 iteration 200 loss: 0.7551843523979187\n",
      "Epoch 94 Training loss 0.789079312220968\n",
      "Epoch:  95 iteration 0 loss: 0.87416298160553\n",
      "Epoch:  95 iteration 100 loss: 0.72548130130767822\n",
      "Epoch:  95 iteration 200 loss: 0.69935665240974426\n",
      "Epoch 95 Training loss 0.7568350895000686\n",
      "Evaluation loss 2.654539817772774\n",
      "BLEU score 0.36587413695108315\n",
      "Epoch:  96 iteration 0 loss: 0.8974683483123779\n",
      "Epoch:  96 iteration 100 loss: 0.8841354209899902\n",
      "Epoch:  96 iteration 200 loss: 0.7351922988891602\n",
      "Epoch 96 Training loss 0.8778813077878384\n",
      "Epoch:  97 iteration 0 loss: 0.764733350276947\n",
      "Epoch:  97 iteration 100 loss: 0.6691336512565613\n",
      "Epoch:  97 iteration 200 loss: 0.819921097755432\n",
      "Epoch 97 Training loss 0.9854978775003133\n",
      "Epoch:  98 iteration 0 loss: 0.9874688865661621\n",
      "Epoch:  98 iteration 100 loss: 0.8414723128318787\n",
      "Epoch:  98 iteration 200 loss: 0.84735159034729\n",
      "Epoch 98 Training loss 0.8864193013196834\n",
      "Epoch:  99 iteration 0 loss: 0.8486050271987915\n",
      "Epoch:  99 iteration 100 loss: 0.8751653171920776\n",
      "Epoch:  99 iteration 200 loss: 0.7814968109130859\n",
      "Epoch 99 Training loss 0.8858599144476884\n"
     ]
    }
   ],
   "source": [
    "model_gru_bahdanau = Seq2Seq(rnn_type='GRU', attn_type='bahdanau')\n",
    "model_gru_bahdanau = model_gru_bahdanau.to(device)\n",
    "optimizer = torch.optim.Adam(model_gru_bahdanau.parameters())\n",
    "train(model_gru_bahdanau, train_batches, num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
